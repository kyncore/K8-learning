
==> Audit <==
|------------|--------------------------------|----------|-------------|---------|---------------------|---------------------|
|  Command   |              Args              | Profile  |    User     | Version |     Start Time      |      End Time       |
|------------|--------------------------------|----------|-------------|---------|---------------------|---------------------|
| docker-env | minikube docker-env            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:20 JST |                     |
| docker-env | minikube docker-env            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:31 JST |                     |
| start      | --driver=qemu                  | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:32 JST | 14 Jul 25 17:33 JST |
|            | --network-plugin=cni           |          |             |         |                     |                     |
|            | --cni=calico                   |          |             |         |                     |                     |
| docker-env | minikube docker-env            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:33 JST | 14 Jul 25 17:33 JST |
| service    | hello-world-service            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:37 JST |                     |
| service    | hello-world-service            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:37 JST |                     |
| start      | --driver=qemu                  | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:38 JST | 14 Jul 25 17:38 JST |
|            | --network-plugin=cni           |          |             |         |                     |                     |
|            | --cni=calico                   |          |             |         |                     |                     |
| service    | hello-world-service            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:38 JST |                     |
| start      | --driver=qemu                  | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:38 JST |                     |
|            | --network=socket_vmnet         |          |             |         |                     |                     |
|            | --network-plugin=cni           |          |             |         |                     |                     |
|            | --cni=calico                   |          |             |         |                     |                     |
| stop       |                                | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:40 JST | 14 Jul 25 17:40 JST |
| delete     |                                | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:42 JST | 14 Jul 25 17:42 JST |
| start      | --driver=qemu                  | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:42 JST |                     |
|            | --network=socket_vmnet         |          |             |         |                     |                     |
|            | --cni=calico                   |          |             |         |                     |                     |
| delete     |                                | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:43 JST | 14 Jul 25 17:43 JST |
| start      | --driver=qemu --network=user   | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:43 JST | 14 Jul 25 17:44 JST |
|            | --cni=calico                   |          |             |         |                     |                     |
| docker-env | minikube docker-env            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:45 JST | 14 Jul 25 17:45 JST |
| service    | hello-world-service            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:48 JST |                     |
| stop       |                                | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:49 JST | 14 Jul 25 17:49 JST |
| delete     | --all                          | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:49 JST | 14 Jul 25 17:49 JST |
| start      | --driver=qemu                  | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:50 JST |                     |
|            | --network=socket_vmnet         |          |             |         |                     |                     |
|            | --cni=calico                   |          |             |         |                     |                     |
| start      | --driver=qemu                  | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:51 JST | 14 Jul 25 17:51 JST |
|            | --network=socket_vmnet         |          |             |         |                     |                     |
|            | --cni=calico                   |          |             |         |                     |                     |
| docker-env | minikube docker-env            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:52 JST | 14 Jul 25 17:52 JST |
| service    | hello-world-service            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:52 JST | 14 Jul 25 17:52 JST |
| stop       |                                | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:54 JST | 14 Jul 25 17:54 JST |
| start      |                                | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:54 JST | 14 Jul 25 17:55 JST |
| service    | hello-world-service            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:57 JST | 14 Jul 25 17:57 JST |
| docker-env | minikube docker-env            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:58 JST | 14 Jul 25 17:58 JST |
| service    | hello-world-service            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 17:58 JST | 14 Jul 25 17:58 JST |
| docker-env | minikube docker-env -u         | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 18:23 JST | 14 Jul 25 18:23 JST |
| service    | hello-world-service            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 18:23 JST | 14 Jul 25 18:23 JST |
| docker-env | minikube docker-env            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 18:25 JST | 14 Jul 25 18:25 JST |
| service    | config-app-service             | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 18:27 JST |                     |
| service    | config-app-service             | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 18:29 JST |                     |
| service    | config-app-service             | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 18:41 JST | 14 Jul 25 18:41 JST |
| service    | storage-app-service            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 18:47 JST |                     |
| docker-env | minikube docker-env            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 18:52 JST | 14 Jul 25 18:52 JST |
| service    | storage-app-service            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 18:52 JST |                     |
| stop       |                                | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 18:52 JST | 14 Jul 25 18:53 JST |
| delete     |                                | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 18:53 JST | 14 Jul 25 18:53 JST |
| start      | --driver=qemu                  | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 18:53 JST | 14 Jul 25 18:53 JST |
|            | --network=socket_vmnet         |          |             |         |                     |                     |
|            | --cni=calico                   |          |             |         |                     |                     |
| docker-env | minikube docker-env            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 18:53 JST | 14 Jul 25 18:53 JST |
| service    | storage-app-service            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 18:54 JST |                     |
| service    | storage-app-service            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 18:55 JST |                     |
| service    | storage-app-service            | minikube | kyawyenaing | v1.36.0 | 14 Jul 25 18:56 JST |                     |
|------------|--------------------------------|----------|-------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/07/14 18:53:06
Running on machine: Kyaws-MacBook-Pro
Binary: Built with gc go1.24.0 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0714 18:53:06.215679   92247 out.go:345] Setting OutFile to fd 1 ...
I0714 18:53:06.216134   92247 out.go:397] isatty.IsTerminal(1) = true
I0714 18:53:06.216137   92247 out.go:358] Setting ErrFile to fd 2...
I0714 18:53:06.216139   92247 out.go:397] isatty.IsTerminal(2) = true
I0714 18:53:06.216271   92247 root.go:338] Updating PATH: /Users/kyawyenaing/.minikube/bin
I0714 18:53:06.216294   92247 oci.go:582] shell is pointing to dockerd inside minikube. will unset to use host
I0714 18:53:06.218413   92247 out.go:352] Setting JSON to false
I0714 18:53:06.243233   92247 start.go:130] hostinfo: {"hostname":"Kyaws-MacBook-Pro.local","uptime":1664213,"bootTime":1750822573,"procs":685,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"15.5","kernelVersion":"24.5.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"9df1bc32-074e-54c9-b02e-7e704dbf5b24"}
W0714 18:53:06.243768   92247 start.go:138] gopshost.Virtualization returned error: not implemented yet
I0714 18:53:06.250762   92247 out.go:177] üòÑ  minikube v1.36.0 on Darwin 15.5 (arm64)
I0714 18:53:06.260402   92247 notify.go:220] Checking for updates...
I0714 18:53:06.265862   92247 out.go:177]     ‚ñ™ MINIKUBE_ACTIVE_DOCKERD=minikube
I0714 18:53:06.271766   92247 driver.go:404] Setting default libvirt URI to qemu:///system
I0714 18:53:06.278070   92247 out.go:177] ‚ú®  Using the qemu2 driver based on user configuration
I0714 18:53:06.285925   92247 start.go:304] selected driver: qemu2
I0714 18:53:06.286224   92247 start.go:908] validating driver "qemu2" against <nil>
I0714 18:53:06.286231   92247 start.go:919] status for qemu2: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0714 18:53:06.286505   92247 start_flags.go:311] no existing cluster config was found, will generate one from the flags 
I0714 18:53:06.287363   92247 start_flags.go:394] Using suggested 4000MB memory alloc based on sys=16384MB, container=0MB
I0714 18:53:06.287594   92247 start_flags.go:958] Wait components to verify : map[apiserver:true system_pods:true]
I0714 18:53:06.288318   92247 cni.go:84] Creating CNI manager for "calico"
I0714 18:53:06.288323   92247 start_flags.go:320] Found "Calico" CNI - setting NetworkPlugin=cni
I0714 18:53:06.288502   92247 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:qemu2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network:socket_vmnet Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/homebrew/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/opt/homebrew/var/run/socket_vmnet StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0714 18:53:06.301277   92247 iso.go:125] acquiring lock: {Name:mk72f7cdfc5c1eae0b2228ffb260f08a886656cc Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0714 18:53:06.310102   92247 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0714 18:53:06.313993   92247 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0714 18:53:06.314007   92247 preload.go:146] Found local preload: /Users/kyawyenaing/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-arm64.tar.lz4
I0714 18:53:06.314081   92247 cache.go:56] Caching tarball of preloaded images
I0714 18:53:06.314362   92247 preload.go:172] Found /Users/kyawyenaing/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0714 18:53:06.314369   92247 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0714 18:53:06.314575   92247 profile.go:143] Saving config to /Users/kyawyenaing/.minikube/profiles/minikube/config.json ...
I0714 18:53:06.314588   92247 lock.go:35] WriteFile acquiring /Users/kyawyenaing/.minikube/profiles/minikube/config.json: {Name:mk03758359c58b9872a526e15aaf51e6052bf94c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0714 18:53:06.314790   92247 start.go:360] acquireMachinesLock for minikube: {Name:mkd6132f011b0abb7bf46eeccede8bb8aeed5e2e Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I0714 18:53:06.314825   92247 start.go:364] duration metric: took 32.709¬µs to acquireMachinesLock for "minikube"
I0714 18:53:06.314831   92247 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.36.0-arm64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:qemu2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network:socket_vmnet Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/homebrew/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/opt/homebrew/var/run/socket_vmnet StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0714 18:53:06.314867   92247 start.go:125] createHost starting for "" (driver="qemu2")
I0714 18:53:06.323032   92247 out.go:235] üî•  Creating qemu2 VM (CPUs=2, Memory=4000MB, Disk=20000MB) ...
I0714 18:53:06.421814   92247 start.go:159] libmachine.API.Create for "minikube" (driver="qemu2")
I0714 18:53:06.421865   92247 client.go:168] LocalClient.Create starting
I0714 18:53:06.422716   92247 main.go:141] libmachine: Reading certificate data from /Users/kyawyenaing/.minikube/certs/ca.pem
I0714 18:53:06.422912   92247 main.go:141] libmachine: Decoding PEM data...
I0714 18:53:06.422921   92247 main.go:141] libmachine: Parsing certificate...
I0714 18:53:06.423292   92247 main.go:141] libmachine: Reading certificate data from /Users/kyawyenaing/.minikube/certs/cert.pem
I0714 18:53:06.423459   92247 main.go:141] libmachine: Decoding PEM data...
I0714 18:53:06.423465   92247 main.go:141] libmachine: Parsing certificate...
I0714 18:53:06.423893   92247 main.go:141] libmachine: Downloading /Users/kyawyenaing/.minikube/cache/boot2docker.iso from file:///Users/kyawyenaing/.minikube/cache/iso/arm64/minikube-v1.36.0-arm64.iso...
I0714 18:53:06.790666   92247 main.go:141] libmachine: Creating SSH key...
I0714 18:53:06.842163   92247 main.go:141] libmachine: Creating Disk image...
I0714 18:53:06.842170   92247 main.go:141] libmachine: Creating 20000 MB hard disk image...
I0714 18:53:06.842429   92247 main.go:141] libmachine: executing: qemu-img convert -f raw -O qcow2 /Users/kyawyenaing/.minikube/machines/minikube/disk.qcow2.raw /Users/kyawyenaing/.minikube/machines/minikube/disk.qcow2
I0714 18:53:06.907461   92247 main.go:141] libmachine: STDOUT: 
I0714 18:53:06.907478   92247 main.go:141] libmachine: STDERR: 
I0714 18:53:06.907562   92247 main.go:141] libmachine: executing: qemu-img resize /Users/kyawyenaing/.minikube/machines/minikube/disk.qcow2 +20000M
I0714 18:53:06.917188   92247 main.go:141] libmachine: STDOUT: Image resized.

I0714 18:53:06.917199   92247 main.go:141] libmachine: STDERR: 
I0714 18:53:06.917217   92247 main.go:141] libmachine: DONE writing to /Users/kyawyenaing/.minikube/machines/minikube/disk.qcow2.raw and /Users/kyawyenaing/.minikube/machines/minikube/disk.qcow2
I0714 18:53:06.917220   92247 main.go:141] libmachine: Starting QEMU VM...
I0714 18:53:06.924874   92247 qemu.go:418] Using hvf for hardware acceleration
I0714 18:53:06.924944   92247 main.go:141] libmachine: executing: /opt/homebrew/opt/socket_vmnet/bin/socket_vmnet_client /opt/homebrew/var/run/socket_vmnet qemu-system-aarch64 -M virt -cpu host -drive file=/opt/homebrew/opt/qemu/share/qemu/edk2-aarch64-code.fd,readonly=on,format=raw,if=pflash -display none -accel hvf -m 4000 -smp 2 -boot d -cdrom /Users/kyawyenaing/.minikube/machines/minikube/boot2docker.iso -qmp unix:/Users/kyawyenaing/.minikube/machines/minikube/monitor,server,nowait -pidfile /Users/kyawyenaing/.minikube/machines/minikube/qemu.pid -device virtio-net-pci,netdev=net0,mac=86:ea:01:61:f6:96 -netdev socket,id=net0,fd=3 -daemonize /Users/kyawyenaing/.minikube/machines/minikube/disk.qcow2
I0714 18:53:07.110865   92247 main.go:141] libmachine: STDOUT: 
I0714 18:53:07.110882   92247 main.go:141] libmachine: STDERR: 
I0714 18:53:07.110912   92247 main.go:141] libmachine: Attempt 0
I0714 18:53:07.110987   92247 main.go:141] libmachine: Searching for 86:ea:01:61:f6:96 in /var/db/dhcpd_leases ...
I0714 18:53:07.111142   92247 main.go:141] libmachine: Found 1 entries in /var/db/dhcpd_leases!
I0714 18:53:07.111158   92247 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.2 HWAddress:de:ec:3e:9b:b6:d7 ID:1,de:ec:3e:9b:b6:d7 Lease:0x6874d379}
I0714 18:53:09.112407   92247 main.go:141] libmachine: Attempt 1
I0714 18:53:09.112446   92247 main.go:141] libmachine: Searching for 86:ea:01:61:f6:96 in /var/db/dhcpd_leases ...
I0714 18:53:09.112597   92247 main.go:141] libmachine: Found 1 entries in /var/db/dhcpd_leases!
I0714 18:53:09.112615   92247 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.2 HWAddress:de:ec:3e:9b:b6:d7 ID:1,de:ec:3e:9b:b6:d7 Lease:0x6874d379}
I0714 18:53:11.113814   92247 main.go:141] libmachine: Attempt 2
I0714 18:53:11.113871   92247 main.go:141] libmachine: Searching for 86:ea:01:61:f6:96 in /var/db/dhcpd_leases ...
I0714 18:53:11.114219   92247 main.go:141] libmachine: Found 1 entries in /var/db/dhcpd_leases!
I0714 18:53:11.114249   92247 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.2 HWAddress:de:ec:3e:9b:b6:d7 ID:1,de:ec:3e:9b:b6:d7 Lease:0x6874d379}
I0714 18:53:13.137763   92247 main.go:141] libmachine: Attempt 3
I0714 18:53:13.137814   92247 main.go:141] libmachine: Searching for 86:ea:01:61:f6:96 in /var/db/dhcpd_leases ...
I0714 18:53:13.145204   92247 main.go:141] libmachine: Found 1 entries in /var/db/dhcpd_leases!
I0714 18:53:13.145217   92247 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.2 HWAddress:de:ec:3e:9b:b6:d7 ID:1,de:ec:3e:9b:b6:d7 Lease:0x6874d379}
I0714 18:53:15.157697   92247 main.go:141] libmachine: Attempt 4
I0714 18:53:15.163974   92247 main.go:141] libmachine: Searching for 86:ea:01:61:f6:96 in /var/db/dhcpd_leases ...
I0714 18:53:15.164219   92247 main.go:141] libmachine: Found 1 entries in /var/db/dhcpd_leases!
I0714 18:53:15.186289   92247 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.2 HWAddress:de:ec:3e:9b:b6:d7 ID:1,de:ec:3e:9b:b6:d7 Lease:0x6874d379}
I0714 18:53:17.193479   92247 main.go:141] libmachine: Attempt 5
I0714 18:53:17.193508   92247 main.go:141] libmachine: Searching for 86:ea:01:61:f6:96 in /var/db/dhcpd_leases ...
I0714 18:53:17.193575   92247 main.go:141] libmachine: Found 1 entries in /var/db/dhcpd_leases!
I0714 18:53:17.193585   92247 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.2 HWAddress:de:ec:3e:9b:b6:d7 ID:1,de:ec:3e:9b:b6:d7 Lease:0x6874d379}
I0714 18:53:19.194918   92247 main.go:141] libmachine: Attempt 6
I0714 18:53:19.195080   92247 main.go:141] libmachine: Searching for 86:ea:01:61:f6:96 in /var/db/dhcpd_leases ...
I0714 18:53:19.195365   92247 main.go:141] libmachine: Found 1 entries in /var/db/dhcpd_leases!
I0714 18:53:19.195395   92247 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.2 HWAddress:de:ec:3e:9b:b6:d7 ID:1,de:ec:3e:9b:b6:d7 Lease:0x6874d379}
I0714 18:53:21.199364   92247 main.go:141] libmachine: Attempt 7
I0714 18:53:21.201020   92247 main.go:141] libmachine: Searching for 86:ea:01:61:f6:96 in /var/db/dhcpd_leases ...
I0714 18:53:21.203122   92247 main.go:141] libmachine: Found 2 entries in /var/db/dhcpd_leases!
I0714 18:53:21.203903   92247 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.3 HWAddress:86:ea:01:61:f6:96 ID:1,86:ea:1:61:f6:96 Lease:0x6874e1a0}
I0714 18:53:21.203908   92247 main.go:141] libmachine: Found match: 86:ea:01:61:f6:96
I0714 18:53:21.203940   92247 main.go:141] libmachine: IP: 192.168.105.3
I0714 18:53:21.203944   92247 main.go:141] libmachine: Waiting for VM to start (ssh -p 22 docker@192.168.105.3)...
I0714 18:53:26.311443   92247 machine.go:93] provisionDockerMachine start ...
I0714 18:53:26.312764   92247 main.go:141] libmachine: Using SSH client type: native
I0714 18:53:26.317911   92247 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100c5fe40] 0x100c62600 <nil>  [] 0s} 192.168.105.3 22 <nil> <nil>}
I0714 18:53:26.317919   92247 main.go:141] libmachine: About to run SSH command:
hostname
I0714 18:53:26.378963   92247 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0714 18:53:26.379161   92247 buildroot.go:166] provisioning hostname "minikube"
I0714 18:53:26.379444   92247 main.go:141] libmachine: Using SSH client type: native
I0714 18:53:26.379668   92247 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100c5fe40] 0x100c62600 <nil>  [] 0s} 192.168.105.3 22 <nil> <nil>}
I0714 18:53:26.379671   92247 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0714 18:53:26.451917   92247 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0714 18:53:26.452097   92247 main.go:141] libmachine: Using SSH client type: native
I0714 18:53:26.452270   92247 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100c5fe40] 0x100c62600 <nil>  [] 0s} 192.168.105.3 22 <nil> <nil>}
I0714 18:53:26.452276   92247 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0714 18:53:26.497553   92247 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0714 18:53:26.497566   92247 buildroot.go:172] set auth options {CertDir:/Users/kyawyenaing/.minikube CaCertPath:/Users/kyawyenaing/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/kyawyenaing/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/kyawyenaing/.minikube/machines/server.pem ServerKeyPath:/Users/kyawyenaing/.minikube/machines/server-key.pem ClientKeyPath:/Users/kyawyenaing/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/kyawyenaing/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/kyawyenaing/.minikube}
I0714 18:53:26.497580   92247 buildroot.go:174] setting up certificates
I0714 18:53:26.497831   92247 provision.go:84] configureAuth start
I0714 18:53:26.497837   92247 provision.go:143] copyHostCerts
I0714 18:53:26.498488   92247 exec_runner.go:144] found /Users/kyawyenaing/.minikube/ca.pem, removing ...
I0714 18:53:26.498623   92247 exec_runner.go:203] rm: /Users/kyawyenaing/.minikube/ca.pem
I0714 18:53:26.498981   92247 exec_runner.go:151] cp: /Users/kyawyenaing/.minikube/certs/ca.pem --> /Users/kyawyenaing/.minikube/ca.pem (1090 bytes)
I0714 18:53:26.499529   92247 exec_runner.go:144] found /Users/kyawyenaing/.minikube/cert.pem, removing ...
I0714 18:53:26.499530   92247 exec_runner.go:203] rm: /Users/kyawyenaing/.minikube/cert.pem
I0714 18:53:26.499594   92247 exec_runner.go:151] cp: /Users/kyawyenaing/.minikube/certs/cert.pem --> /Users/kyawyenaing/.minikube/cert.pem (1135 bytes)
I0714 18:53:26.499886   92247 exec_runner.go:144] found /Users/kyawyenaing/.minikube/key.pem, removing ...
I0714 18:53:26.499888   92247 exec_runner.go:203] rm: /Users/kyawyenaing/.minikube/key.pem
I0714 18:53:26.499941   92247 exec_runner.go:151] cp: /Users/kyawyenaing/.minikube/certs/key.pem --> /Users/kyawyenaing/.minikube/key.pem (1675 bytes)
I0714 18:53:26.500326   92247 provision.go:117] generating server cert: /Users/kyawyenaing/.minikube/machines/server.pem ca-key=/Users/kyawyenaing/.minikube/certs/ca.pem private-key=/Users/kyawyenaing/.minikube/certs/ca-key.pem org=kyawyenaing.minikube san=[127.0.0.1 192.168.105.3 localhost minikube]
I0714 18:53:26.659676   92247 provision.go:177] copyRemoteCerts
I0714 18:53:26.660439   92247 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0714 18:53:26.660455   92247 sshutil.go:53] new ssh client: &{IP:192.168.105.3 Port:22 SSHKeyPath:/Users/kyawyenaing/.minikube/machines/minikube/id_rsa Username:docker}
I0714 18:53:26.684579   92247 ssh_runner.go:362] scp /Users/kyawyenaing/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1090 bytes)
I0714 18:53:26.694168   92247 ssh_runner.go:362] scp /Users/kyawyenaing/.minikube/machines/server.pem --> /etc/docker/server.pem (1192 bytes)
I0714 18:53:26.702393   92247 ssh_runner.go:362] scp /Users/kyawyenaing/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0714 18:53:26.715026   92247 provision.go:87] duration metric: took 216.974375ms to configureAuth
I0714 18:53:26.715036   92247 buildroot.go:189] setting minikube options for container-runtime
I0714 18:53:26.715471   92247 config.go:182] Loaded profile config "minikube": Driver=qemu2, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0714 18:53:26.715544   92247 main.go:141] libmachine: Using SSH client type: native
I0714 18:53:26.715661   92247 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100c5fe40] 0x100c62600 <nil>  [] 0s} 192.168.105.3 22 <nil> <nil>}
I0714 18:53:26.715664   92247 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0714 18:53:26.758068   92247 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I0714 18:53:26.758277   92247 buildroot.go:70] root file system type: tmpfs
I0714 18:53:26.758650   92247 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0714 18:53:26.758736   92247 main.go:141] libmachine: Using SSH client type: native
I0714 18:53:26.758894   92247 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100c5fe40] 0x100c62600 <nil>  [] 0s} 192.168.105.3 22 <nil> <nil>}
I0714 18:53:26.758919   92247 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=qemu2 --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0714 18:53:26.803735   92247 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=qemu2 --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0714 18:53:26.803834   92247 main.go:141] libmachine: Using SSH client type: native
I0714 18:53:26.803991   92247 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100c5fe40] 0x100c62600 <nil>  [] 0s} 192.168.105.3 22 <nil> <nil>}
I0714 18:53:26.803996   92247 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0714 18:53:27.301967   92247 main.go:141] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink '/etc/systemd/system/multi-user.target.wants/docker.service' ‚Üí '/usr/lib/systemd/system/docker.service'.

I0714 18:53:27.301976   92247 machine.go:96] duration metric: took 990.507ms to provisionDockerMachine
I0714 18:53:27.301982   92247 client.go:171] duration metric: took 20.880089208s to LocalClient.Create
I0714 18:53:27.302001   92247 start.go:167] duration metric: took 20.880170333s to libmachine.API.Create "minikube"
I0714 18:53:27.302004   92247 start.go:293] postStartSetup for "minikube" (driver="qemu2")
I0714 18:53:27.302010   92247 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0714 18:53:27.302114   92247 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0714 18:53:27.302121   92247 sshutil.go:53] new ssh client: &{IP:192.168.105.3 Port:22 SSHKeyPath:/Users/kyawyenaing/.minikube/machines/minikube/id_rsa Username:docker}
I0714 18:53:27.388432   92247 ssh_runner.go:195] Run: cat /etc/os-release
I0714 18:53:27.389721   92247 info.go:137] Remote host: Buildroot 2025.02
I0714 18:53:27.389908   92247 filesync.go:126] Scanning /Users/kyawyenaing/.minikube/addons for local assets ...
I0714 18:53:27.390065   92247 filesync.go:126] Scanning /Users/kyawyenaing/.minikube/files for local assets ...
I0714 18:53:27.390126   92247 start.go:296] duration metric: took 88.11875ms for postStartSetup
I0714 18:53:27.390686   92247 profile.go:143] Saving config to /Users/kyawyenaing/.minikube/profiles/minikube/config.json ...
I0714 18:53:27.390870   92247 start.go:128] duration metric: took 21.075975416s to createHost
I0714 18:53:27.390922   92247 main.go:141] libmachine: Using SSH client type: native
I0714 18:53:27.391052   92247 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100c5fe40] 0x100c62600 <nil>  [] 0s} 192.168.105.3 22 <nil> <nil>}
I0714 18:53:27.391054   92247 main.go:141] libmachine: About to run SSH command:
date +%s.%N
I0714 18:53:27.432700   92247 main.go:141] libmachine: SSH cmd err, output: <nil>: 1752486806.977217170

I0714 18:53:27.432709   92247 fix.go:216] guest clock: 1752486806.977217170
I0714 18:53:27.432712   92247 fix.go:229] Guest: 2025-07-14 18:53:26.97721717 +0900 JST Remote: 2025-07-14 18:53:27.390872 +0900 JST m=+21.200174876 (delta=-413.65483ms)
I0714 18:53:27.432725   92247 fix.go:200] guest clock delta is within tolerance: -413.65483ms
I0714 18:53:27.432727   92247 start.go:83] releasing machines lock for "minikube", held for 21.117874125s
I0714 18:53:27.433856   92247 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0714 18:53:27.433893   92247 ssh_runner.go:195] Run: cat /version.json
I0714 18:53:27.433898   92247 sshutil.go:53] new ssh client: &{IP:192.168.105.3 Port:22 SSHKeyPath:/Users/kyawyenaing/.minikube/machines/minikube/id_rsa Username:docker}
I0714 18:53:27.434026   92247 sshutil.go:53] new ssh client: &{IP:192.168.105.3 Port:22 SSHKeyPath:/Users/kyawyenaing/.minikube/machines/minikube/id_rsa Username:docker}
W0714 18:53:27.528388   92247 out.go:270] ‚ùó  Image was not built for the current minikube version. To resolve this you can delete and recreate your minikube cluster using the latest images. Expected minikube version: v1.35.0 -> Actual minikube version: v1.36.0
I0714 18:53:27.528975   92247 ssh_runner.go:195] Run: systemctl --version
I0714 18:53:27.533682   92247 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0714 18:53:27.536647   92247 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0714 18:53:27.536709   92247 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0714 18:53:27.545086   92247 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0714 18:53:27.545552   92247 start.go:495] detecting cgroup driver to use...
I0714 18:53:27.546466   92247 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0714 18:53:27.553512   92247 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0714 18:53:27.557597   92247 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0714 18:53:27.561213   92247 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0714 18:53:27.561258   92247 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0714 18:53:27.564931   92247 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0714 18:53:27.568195   92247 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0714 18:53:27.571456   92247 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0714 18:53:27.575403   92247 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0714 18:53:27.579342   92247 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0714 18:53:27.582825   92247 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0714 18:53:27.586416   92247 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0714 18:53:27.590125   92247 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0714 18:53:27.593087   92247 crio.go:166] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 1
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I0714 18:53:27.593138   92247 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I0714 18:53:27.596973   92247 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0714 18:53:27.600399   92247 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0714 18:53:27.698325   92247 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0714 18:53:27.710387   92247 start.go:495] detecting cgroup driver to use...
I0714 18:53:27.711618   92247 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0714 18:53:27.723349   92247 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0714 18:53:27.728742   92247 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0714 18:53:27.738134   92247 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0714 18:53:27.742662   92247 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0714 18:53:27.747072   92247 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I0714 18:53:27.785948   92247 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0714 18:53:27.791046   92247 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0714 18:53:27.796915   92247 ssh_runner.go:195] Run: which cri-dockerd
I0714 18:53:27.797949   92247 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0714 18:53:27.801145   92247 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0714 18:53:27.806670   92247 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0714 18:53:27.919017   92247 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0714 18:53:28.034669   92247 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0714 18:53:28.034991   92247 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0714 18:53:28.041630   92247 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0714 18:53:28.047275   92247 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0714 18:53:28.148778   92247 ssh_runner.go:195] Run: sudo systemctl restart docker
I0714 18:53:29.306733   92247 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.157937667s)
I0714 18:53:29.306843   92247 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0714 18:53:29.312391   92247 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0714 18:53:29.318642   92247 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0714 18:53:29.323247   92247 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0714 18:53:29.428596   92247 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0714 18:53:29.515367   92247 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0714 18:53:29.617998   92247 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0714 18:53:29.629430   92247 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0714 18:53:29.636226   92247 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0714 18:53:29.735668   92247 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0714 18:53:29.764835   92247 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0714 18:53:29.770086   92247 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0714 18:53:29.770952   92247 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0714 18:53:29.772893   92247 start.go:563] Will wait 60s for crictl version
I0714 18:53:29.772938   92247 ssh_runner.go:195] Run: which crictl
I0714 18:53:29.774139   92247 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0714 18:53:29.797202   92247 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.0.4
RuntimeApiVersion:  v1
I0714 18:53:29.797285   92247 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0714 18:53:29.809096   92247 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0714 18:53:29.824707   92247 out.go:235] üê≥  Preparing Kubernetes v1.33.1 on Docker 28.0.4 ...
I0714 18:53:29.825543   92247 ssh_runner.go:195] Run: grep 192.168.105.1	host.minikube.internal$ /etc/hosts
I0714 18:53:29.831669   92247 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.105.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0714 18:53:29.845934   92247 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.36.0-arm64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:qemu2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico} Nodes:[{Name: IP:192.168.105.3 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network:socket_vmnet Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/homebrew/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/opt/homebrew/var/run/socket_vmnet StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0714 18:53:29.846722   92247 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0714 18:53:29.846804   92247 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0714 18:53:29.851308   92247 docker.go:702] Got preloaded images: 
I0714 18:53:29.851312   92247 docker.go:708] registry.k8s.io/kube-apiserver:v1.33.1 wasn't preloaded
I0714 18:53:29.851386   92247 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I0714 18:53:29.856145   92247 ssh_runner.go:195] Run: which lz4
I0714 18:53:29.857567   92247 ssh_runner.go:195] Run: stat -c "%s %y" /preloaded.tar.lz4
I0714 18:53:29.858750   92247 ssh_runner.go:352] existence check for /preloaded.tar.lz4: stat -c "%s %y" /preloaded.tar.lz4: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/preloaded.tar.lz4': No such file or directory
I0714 18:53:29.858763   92247 ssh_runner.go:362] scp /Users/kyawyenaing/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-arm64.tar.lz4 --> /preloaded.tar.lz4 (343036662 bytes)
I0714 18:53:31.081266   92247 docker.go:666] duration metric: took 1.223742666s to copy over tarball
I0714 18:53:31.081372   92247 ssh_runner.go:195] Run: sudo tar --xattrs --xattrs-include security.capability -I lz4 -C /var -xf /preloaded.tar.lz4
I0714 18:53:32.333040   92247 ssh_runner.go:235] Completed: sudo tar --xattrs --xattrs-include security.capability -I lz4 -C /var -xf /preloaded.tar.lz4: (1.251637084s)
I0714 18:53:32.333083   92247 ssh_runner.go:146] rm: /preloaded.tar.lz4
I0714 18:53:32.350520   92247 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I0714 18:53:32.354231   92247 ssh_runner.go:362] scp memory --> /var/lib/docker/image/overlay2/repositories.json (2631 bytes)
I0714 18:53:32.360541   92247 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0714 18:53:32.366800   92247 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0714 18:53:32.479737   92247 ssh_runner.go:195] Run: sudo systemctl restart docker
I0714 18:53:33.960896   92247 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.481141833s)
I0714 18:53:33.961379   92247 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0714 18:53:33.973108   92247 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0714 18:53:33.973118   92247 cache_images.go:84] Images are preloaded, skipping loading
I0714 18:53:33.974734   92247 kubeadm.go:926] updating node { 192.168.105.3 8443 v1.33.1 docker true true} ...
I0714 18:53:33.979953   92247 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.105.3

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico}
I0714 18:53:33.980036   92247 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0714 18:53:34.015156   92247 cni.go:84] Creating CNI manager for "calico"
I0714 18:53:34.016748   92247 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0714 18:53:34.018325   92247 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.105.3 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.105.3"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.105.3 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0714 18:53:34.020013   92247 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.105.3
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.105.3"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.105.3"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0714 18:53:34.020157   92247 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0714 18:53:34.023875   92247 binaries.go:44] Found k8s binaries, skipping transfer
I0714 18:53:34.023942   92247 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0714 18:53:34.027022   92247 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (308 bytes)
I0714 18:53:34.032335   92247 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0714 18:53:34.037541   92247 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2289 bytes)
I0714 18:53:34.042936   92247 ssh_runner.go:195] Run: grep 192.168.105.3	control-plane.minikube.internal$ /etc/hosts
I0714 18:53:34.044248   92247 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.105.3	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0714 18:53:34.048068   92247 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0714 18:53:34.148094   92247 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0714 18:53:34.168995   92247 certs.go:68] Setting up /Users/kyawyenaing/.minikube/profiles/minikube for IP: 192.168.105.3
I0714 18:53:34.169011   92247 certs.go:194] generating shared ca certs ...
I0714 18:53:34.169032   92247 certs.go:226] acquiring lock for ca certs: {Name:mk84336bcf06f19fea79d1af0aacfa4f70c71597 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0714 18:53:34.170138   92247 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/kyawyenaing/.minikube/ca.key
I0714 18:53:34.170318   92247 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/kyawyenaing/.minikube/proxy-client-ca.key
I0714 18:53:34.170328   92247 certs.go:256] generating profile certs ...
I0714 18:53:34.170368   92247 certs.go:363] generating signed profile cert for "minikube-user": /Users/kyawyenaing/.minikube/profiles/minikube/client.key
I0714 18:53:34.170639   92247 crypto.go:68] Generating cert /Users/kyawyenaing/.minikube/profiles/minikube/client.crt with IP's: []
I0714 18:53:34.308393   92247 crypto.go:156] Writing cert to /Users/kyawyenaing/.minikube/profiles/minikube/client.crt ...
I0714 18:53:34.308406   92247 lock.go:35] WriteFile acquiring /Users/kyawyenaing/.minikube/profiles/minikube/client.crt: {Name:mk7cca9b4752b645f0bc5026fc1d8372fbd4565e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0714 18:53:34.308649   92247 crypto.go:164] Writing key to /Users/kyawyenaing/.minikube/profiles/minikube/client.key ...
I0714 18:53:34.308651   92247 lock.go:35] WriteFile acquiring /Users/kyawyenaing/.minikube/profiles/minikube/client.key: {Name:mkd4e131f0f07c6d0077621ce33bac2ec61cbf03 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0714 18:53:34.308774   92247 certs.go:363] generating signed profile cert for "minikube": /Users/kyawyenaing/.minikube/profiles/minikube/apiserver.key.5a7a8cab
I0714 18:53:34.308782   92247 crypto.go:68] Generating cert /Users/kyawyenaing/.minikube/profiles/minikube/apiserver.crt.5a7a8cab with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.105.3]
I0714 18:53:34.616095   92247 crypto.go:156] Writing cert to /Users/kyawyenaing/.minikube/profiles/minikube/apiserver.crt.5a7a8cab ...
I0714 18:53:34.616110   92247 lock.go:35] WriteFile acquiring /Users/kyawyenaing/.minikube/profiles/minikube/apiserver.crt.5a7a8cab: {Name:mkc1ad0637a253ce60b67693190483da94241e73 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0714 18:53:34.616392   92247 crypto.go:164] Writing key to /Users/kyawyenaing/.minikube/profiles/minikube/apiserver.key.5a7a8cab ...
I0714 18:53:34.616394   92247 lock.go:35] WriteFile acquiring /Users/kyawyenaing/.minikube/profiles/minikube/apiserver.key.5a7a8cab: {Name:mkef9ef5a7e8f9afee4fb2b868bc640d95a84940 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0714 18:53:34.618053   92247 certs.go:381] copying /Users/kyawyenaing/.minikube/profiles/minikube/apiserver.crt.5a7a8cab -> /Users/kyawyenaing/.minikube/profiles/minikube/apiserver.crt
I0714 18:53:34.618163   92247 certs.go:385] copying /Users/kyawyenaing/.minikube/profiles/minikube/apiserver.key.5a7a8cab -> /Users/kyawyenaing/.minikube/profiles/minikube/apiserver.key
I0714 18:53:34.618258   92247 certs.go:363] generating signed profile cert for "aggregator": /Users/kyawyenaing/.minikube/profiles/minikube/proxy-client.key
I0714 18:53:34.618266   92247 crypto.go:68] Generating cert /Users/kyawyenaing/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0714 18:53:34.677756   92247 crypto.go:156] Writing cert to /Users/kyawyenaing/.minikube/profiles/minikube/proxy-client.crt ...
I0714 18:53:34.677764   92247 lock.go:35] WriteFile acquiring /Users/kyawyenaing/.minikube/profiles/minikube/proxy-client.crt: {Name:mk08951438d25644085a91b082d2276837eb1359 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0714 18:53:34.678003   92247 crypto.go:164] Writing key to /Users/kyawyenaing/.minikube/profiles/minikube/proxy-client.key ...
I0714 18:53:34.678005   92247 lock.go:35] WriteFile acquiring /Users/kyawyenaing/.minikube/profiles/minikube/proxy-client.key: {Name:mk6f9a8fdcb25c8b17af799021d6b77968dff42e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0714 18:53:34.678420   92247 certs.go:484] found cert: /Users/kyawyenaing/.minikube/certs/ca-key.pem (1679 bytes)
I0714 18:53:34.678573   92247 certs.go:484] found cert: /Users/kyawyenaing/.minikube/certs/ca.pem (1090 bytes)
I0714 18:53:34.678605   92247 certs.go:484] found cert: /Users/kyawyenaing/.minikube/certs/cert.pem (1135 bytes)
I0714 18:53:34.678668   92247 certs.go:484] found cert: /Users/kyawyenaing/.minikube/certs/key.pem (1675 bytes)
I0714 18:53:34.693816   92247 ssh_runner.go:362] scp /Users/kyawyenaing/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0714 18:53:34.708539   92247 ssh_runner.go:362] scp /Users/kyawyenaing/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0714 18:53:34.733530   92247 ssh_runner.go:362] scp /Users/kyawyenaing/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0714 18:53:34.742695   92247 ssh_runner.go:362] scp /Users/kyawyenaing/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0714 18:53:34.751421   92247 ssh_runner.go:362] scp /Users/kyawyenaing/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0714 18:53:34.760646   92247 ssh_runner.go:362] scp /Users/kyawyenaing/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0714 18:53:34.768717   92247 ssh_runner.go:362] scp /Users/kyawyenaing/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0714 18:53:34.777532   92247 ssh_runner.go:362] scp /Users/kyawyenaing/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0714 18:53:34.786918   92247 ssh_runner.go:362] scp /Users/kyawyenaing/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0714 18:53:34.794966   92247 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0714 18:53:34.808404   92247 ssh_runner.go:195] Run: openssl version
I0714 18:53:34.811454   92247 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0714 18:53:34.816269   92247 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0714 18:53:34.817763   92247 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jul 14 08:33 /usr/share/ca-certificates/minikubeCA.pem
I0714 18:53:34.817816   92247 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0714 18:53:34.821759   92247 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0714 18:53:34.830504   92247 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0714 18:53:34.833112   92247 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0714 18:53:34.834034   92247 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.36.0-arm64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:qemu2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico} Nodes:[{Name: IP:192.168.105.3 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network:socket_vmnet Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/homebrew/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/opt/homebrew/var/run/socket_vmnet StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0714 18:53:34.834172   92247 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0714 18:53:34.841935   92247 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0714 18:53:34.846085   92247 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0714 18:53:34.849971   92247 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0714 18:53:34.854173   92247 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0714 18:53:34.854181   92247 kubeadm.go:157] found existing configuration files:

I0714 18:53:34.854256   92247 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0714 18:53:34.859833   92247 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0714 18:53:34.859918   92247 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0714 18:53:34.863672   92247 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0714 18:53:34.867371   92247 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0714 18:53:34.867456   92247 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0714 18:53:34.870956   92247 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0714 18:53:34.874353   92247 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0714 18:53:34.874425   92247 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0714 18:53:34.879043   92247 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0714 18:53:34.883091   92247 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0714 18:53:34.883183   92247 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0714 18:53:34.887547   92247 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem"
I0714 18:53:34.939955   92247 kubeadm.go:310] [init] Using Kubernetes version: v1.33.1
I0714 18:53:34.939996   92247 kubeadm.go:310] [preflight] Running pre-flight checks
I0714 18:53:35.091488   92247 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0714 18:53:35.093326   92247 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0714 18:53:35.094434   92247 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0714 18:53:35.181135   92247 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0714 18:53:35.222834   92247 out.go:235]     ‚ñ™ Generating certificates and keys ...
I0714 18:53:35.296149   92247 kubeadm.go:310] [certs] Using existing ca certificate authority
I0714 18:53:35.296264   92247 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0714 18:53:35.596139   92247 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0714 18:53:35.657493   92247 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0714 18:53:35.742930   92247 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0714 18:53:35.923929   92247 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0714 18:53:36.133996   92247 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0714 18:53:36.134059   92247 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.105.3 127.0.0.1 ::1]
I0714 18:53:36.239746   92247 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0714 18:53:36.239801   92247 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.105.3 127.0.0.1 ::1]
I0714 18:53:36.710855   92247 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0714 18:53:36.815354   92247 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0714 18:53:36.887030   92247 kubeadm.go:310] [certs] Generating "sa" key and public key
I0714 18:53:36.887066   92247 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0714 18:53:37.054000   92247 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0714 18:53:37.289767   92247 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0714 18:53:37.359194   92247 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0714 18:53:37.451249   92247 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0714 18:53:37.642734   92247 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0714 18:53:37.642977   92247 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0714 18:53:37.644418   92247 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0714 18:53:37.649739   92247 out.go:235]     ‚ñ™ Booting up control plane ...
I0714 18:53:37.649836   92247 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0714 18:53:37.649869   92247 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0714 18:53:37.649896   92247 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0714 18:53:37.651749   92247 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0714 18:53:37.654126   92247 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0714 18:53:37.654146   92247 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0714 18:53:37.773252   92247 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0714 18:53:37.773318   92247 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0714 18:53:38.274915   92247 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 504.059042ms
I0714 18:53:38.276280   92247 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0714 18:53:38.276323   92247 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.105.3:8443/livez
I0714 18:53:38.276381   92247 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0714 18:53:38.276431   92247 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0714 18:53:39.934900   92247 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 1.652712168s
I0714 18:53:40.485308   92247 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 2.208787376s
I0714 18:53:41.778513   92247 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 3.502013085s
I0714 18:53:41.790288   92247 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0714 18:53:41.795030   92247 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0714 18:53:41.802717   92247 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0714 18:53:41.802826   92247 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0714 18:53:41.806161   92247 kubeadm.go:310] [bootstrap-token] Using token: mhuc3y.2qxdewvfa0ckza89
I0714 18:53:41.812284   92247 out.go:235]     ‚ñ™ Configuring RBAC rules ...
I0714 18:53:41.812376   92247 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0714 18:53:41.813300   92247 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0714 18:53:41.821121   92247 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0714 18:53:41.822543   92247 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0714 18:53:41.822785   92247 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0714 18:53:41.823814   92247 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0714 18:53:42.186127   92247 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0714 18:53:42.591360   92247 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0714 18:53:43.185825   92247 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0714 18:53:43.185854   92247 kubeadm.go:310] 
I0714 18:53:43.185881   92247 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0714 18:53:43.185882   92247 kubeadm.go:310] 
I0714 18:53:43.185921   92247 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0714 18:53:43.185924   92247 kubeadm.go:310] 
I0714 18:53:43.185952   92247 kubeadm.go:310]   mkdir -p $HOME/.kube
I0714 18:53:43.186015   92247 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0714 18:53:43.186036   92247 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0714 18:53:43.186038   92247 kubeadm.go:310] 
I0714 18:53:43.186058   92247 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0714 18:53:43.186059   92247 kubeadm.go:310] 
I0714 18:53:43.186082   92247 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0714 18:53:43.186084   92247 kubeadm.go:310] 
I0714 18:53:43.186104   92247 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0714 18:53:43.186143   92247 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0714 18:53:43.186180   92247 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0714 18:53:43.186181   92247 kubeadm.go:310] 
I0714 18:53:43.186214   92247 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0714 18:53:43.186249   92247 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0714 18:53:43.186255   92247 kubeadm.go:310] 
I0714 18:53:43.186300   92247 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token mhuc3y.2qxdewvfa0ckza89 \
I0714 18:53:43.186349   92247 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:be4cc38cdd7e514594b3abe711438d98572f25f7eef752fabb169e114fa0d861 \
I0714 18:53:43.186361   92247 kubeadm.go:310] 	--control-plane 
I0714 18:53:43.186363   92247 kubeadm.go:310] 
I0714 18:53:43.186397   92247 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0714 18:53:43.186447   92247 kubeadm.go:310] 
I0714 18:53:43.186490   92247 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token mhuc3y.2qxdewvfa0ckza89 \
I0714 18:53:43.186539   92247 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:be4cc38cdd7e514594b3abe711438d98572f25f7eef752fabb169e114fa0d861 
I0714 18:53:43.188432   92247 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0714 18:53:43.188537   92247 cni.go:84] Creating CNI manager for "calico"
I0714 18:53:43.197118   92247 out.go:177] üîó  Configuring Calico (Container Networking Interface) ...
I0714 18:53:43.221934   92247 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.33.1/kubectl ...
I0714 18:53:43.221949   92247 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (537473 bytes)
I0714 18:53:43.236944   92247 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0714 18:53:44.076535   92247 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0714 18:53:44.076759   92247 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0714 18:53:44.076913   92247 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_07_14T18_53_44_0700 minikube.k8s.io/version=v1.36.0 minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0714 18:53:44.087069   92247 ops.go:34] apiserver oom_adj: -16
I0714 18:53:44.118359   92247 kubeadm.go:1105] duration metric: took 41.800375ms to wait for elevateKubeSystemPrivileges
I0714 18:53:44.128454   92247 kubeadm.go:394] duration metric: took 9.294418917s to StartCluster
I0714 18:53:44.128473   92247 settings.go:142] acquiring lock: {Name:mkc2da91acc08bfbb811f1033986b054f964fdd7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0714 18:53:44.128618   92247 settings.go:150] Updating kubeconfig:  /Users/kyawyenaing/.kube/config
I0714 18:53:44.131253   92247 lock.go:35] WriteFile acquiring /Users/kyawyenaing/.kube/config: {Name:mkc10632a8a64a1e94a9798a4a79c3cebb94c2d9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0714 18:53:44.131868   92247 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0714 18:53:44.131978   92247 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.105.3 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0714 18:53:44.132188   92247 config.go:182] Loaded profile config "minikube": Driver=qemu2, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0714 18:53:44.132427   92247 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0714 18:53:44.132558   92247 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0714 18:53:44.132572   92247 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0714 18:53:44.132599   92247 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0714 18:53:44.132732   92247 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0714 18:53:44.133036   92247 host.go:66] Checking if "minikube" exists ...
I0714 18:53:44.137912   92247 out.go:177] üîé  Verifying Kubernetes components...
I0714 18:53:44.146003   92247 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0714 18:53:44.146044   92247 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0714 18:53:44.150320   92247 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0714 18:53:44.150324   92247 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0714 18:53:44.150337   92247 sshutil.go:53] new ssh client: &{IP:192.168.105.3 Port:22 SSHKeyPath:/Users/kyawyenaing/.minikube/machines/minikube/id_rsa Username:docker}
I0714 18:53:44.158926   92247 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0714 18:53:44.158948   92247 host.go:66] Checking if "minikube" exists ...
I0714 18:53:44.159864   92247 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0714 18:53:44.159873   92247 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0714 18:53:44.159886   92247 sshutil.go:53] new ssh client: &{IP:192.168.105.3 Port:22 SSHKeyPath:/Users/kyawyenaing/.minikube/machines/minikube/id_rsa Username:docker}
I0714 18:53:44.184793   92247 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.105.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0714 18:53:44.274111   92247 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0714 18:53:44.300020   92247 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0714 18:53:44.314373   92247 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0714 18:53:44.384916   92247 start.go:971] {"host.minikube.internal": 192.168.105.1} host record injected into CoreDNS's ConfigMap
I0714 18:53:44.385583   92247 api_server.go:52] waiting for apiserver process to appear ...
I0714 18:53:44.385648   92247 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0714 18:53:44.484459   92247 api_server.go:72] duration metric: took 352.467209ms to wait for apiserver process to appear ...
I0714 18:53:44.484477   92247 api_server.go:88] waiting for apiserver healthz status ...
I0714 18:53:44.484488   92247 api_server.go:253] Checking apiserver healthz at https://192.168.105.3:8443/healthz ...
I0714 18:53:44.487986   92247 api_server.go:279] https://192.168.105.3:8443/healthz returned 200:
ok
I0714 18:53:44.488608   92247 api_server.go:141] control plane version: v1.33.1
I0714 18:53:44.488614   92247 api_server.go:131] duration metric: took 4.134542ms to wait for apiserver health ...
I0714 18:53:44.488747   92247 system_pods.go:43] waiting for kube-system pods to appear ...
I0714 18:53:44.492680   92247 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0714 18:53:44.494699   92247 system_pods.go:59] 5 kube-system pods found
I0714 18:53:44.494709   92247 system_pods.go:61] "etcd-minikube" [3ac840d7-3662-4d47-b3cb-41d4389db5bb] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0714 18:53:44.494713   92247 system_pods.go:61] "kube-apiserver-minikube" [ffcf1675-16cd-453f-a73e-9c937cd5054b] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0714 18:53:44.494716   92247 system_pods.go:61] "kube-controller-manager-minikube" [08c0e95d-2436-4175-adb5-88eed35bd773] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0714 18:53:44.494718   92247 system_pods.go:61] "kube-scheduler-minikube" [63a90448-f62f-40ad-ba49-fd72ae0b86a9] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0714 18:53:44.494719   92247 system_pods.go:61] "storage-provisioner" [be712e18-f530-429c-b888-681e607dc27d] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0714 18:53:44.494722   92247 system_pods.go:74] duration metric: took 5.973042ms to wait for pod list to return data ...
I0714 18:53:44.494727   92247 kubeadm.go:578] duration metric: took 362.739542ms to wait for: map[apiserver:true system_pods:true]
I0714 18:53:44.494735   92247 node_conditions.go:102] verifying NodePressure condition ...
I0714 18:53:44.497186   92247 node_conditions.go:122] node storage ephemeral capacity is 17734596Ki
I0714 18:53:44.497193   92247 node_conditions.go:123] node cpu capacity is 2
I0714 18:53:44.497436   92247 node_conditions.go:105] duration metric: took 2.603ms to run NodePressure ...
I0714 18:53:44.497441   92247 start.go:241] waiting for startup goroutines ...
I0714 18:53:44.500737   92247 addons.go:514] duration metric: took 368.564583ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0714 18:53:44.888428   92247 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0714 18:53:44.888443   92247 start.go:246] waiting for cluster config update ...
I0714 18:53:44.888451   92247 start.go:255] writing updated cluster config ...
I0714 18:53:44.888857   92247 ssh_runner.go:195] Run: rm -f paused
I0714 18:53:45.079901   92247 start.go:607] kubectl: 1.33.2, cluster: 1.33.1 (minor skew: 0)
I0714 18:53:45.084527   92247 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jul 14 09:54:20 minikube dockerd[1582]: time="2025-07-14T09:54:20.906418505Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jul 14 09:54:33 minikube dockerd[1582]: time="2025-07-14T09:54:33.794189831Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jul 14 09:54:33 minikube dockerd[1582]: time="2025-07-14T09:54:33.794262624Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jul 14 09:55:00 minikube dockerd[1582]: time="2025-07-14T09:55:00.883558751Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jul 14 09:55:00 minikube dockerd[1582]: time="2025-07-14T09:55:00.883646376Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jul 14 09:55:51 minikube dockerd[1582]: time="2025-07-14T09:55:51.875342888Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jul 14 09:55:51 minikube dockerd[1582]: time="2025-07-14T09:55:51.878586807Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jul 14 09:56:43 minikube cri-dockerd[1448]: 2025-07-14 09:56:42.942 [INFO][8159] cni-plugin/k8s.go 640: Cleaning up netns ContainerID="e26a9a900cfa9ccc906f8c6a7819014f50531adfeeae7d5ef67d9dc6d0683210"
Jul 14 09:56:43 minikube cri-dockerd[1448]: 2025-07-14 09:56:42.943 [INFO][8159] cni-plugin/dataplane_linux.go 559: Deleting workload's device in netns. ContainerID="e26a9a900cfa9ccc906f8c6a7819014f50531adfeeae7d5ef67d9dc6d0683210" iface="eth0" netns="/proc/5842/ns/net"
Jul 14 09:56:43 minikube cri-dockerd[1448]: 2025-07-14 09:56:42.943 [INFO][8159] cni-plugin/dataplane_linux.go 570: Entered netns, deleting veth. ContainerID="e26a9a900cfa9ccc906f8c6a7819014f50531adfeeae7d5ef67d9dc6d0683210" iface="eth0" netns="/proc/5842/ns/net"
Jul 14 09:56:43 minikube cri-dockerd[1448]: 2025-07-14 09:56:42.958 [INFO][8159] cni-plugin/dataplane_linux.go 604: Deleted device in netns. ContainerID="e26a9a900cfa9ccc906f8c6a7819014f50531adfeeae7d5ef67d9dc6d0683210" after=15.422086ms iface="eth0" netns="/proc/5842/ns/net"
Jul 14 09:56:43 minikube cri-dockerd[1448]: 2025-07-14 09:56:42.958 [INFO][8159] cni-plugin/k8s.go 647: Releasing IP address(es) ContainerID="e26a9a900cfa9ccc906f8c6a7819014f50531adfeeae7d5ef67d9dc6d0683210"
Jul 14 09:56:43 minikube cri-dockerd[1448]: 2025-07-14 09:56:42.958 [INFO][8159] cni-plugin/utils.go 188: Calico CNI releasing IP address ContainerID="e26a9a900cfa9ccc906f8c6a7819014f50531adfeeae7d5ef67d9dc6d0683210"
Jul 14 09:56:43 minikube cri-dockerd[1448]: 2025-07-14 09:56:42.986 [INFO][8174] ipam/ipam_plugin.go 412: Releasing address using handleID ContainerID="e26a9a900cfa9ccc906f8c6a7819014f50531adfeeae7d5ef67d9dc6d0683210" HandleID="k8s-pod-network.e26a9a900cfa9ccc906f8c6a7819014f50531adfeeae7d5ef67d9dc6d0683210" Workload="minikube-k8s-storage--app--deployment--58d8dfb886--8ssbz-eth0"
Jul 14 09:56:43 minikube cri-dockerd[1448]: 2025-07-14 09:56:42.987 [INFO][8174] ipam/ipam_plugin.go 353: About to acquire host-wide IPAM lock.
Jul 14 09:56:43 minikube cri-dockerd[1448]: 2025-07-14 09:56:42.987 [INFO][8174] ipam/ipam_plugin.go 368: Acquired host-wide IPAM lock.
Jul 14 09:56:43 minikube cri-dockerd[1448]: 2025-07-14 09:56:43.008 [INFO][8174] ipam/ipam_plugin.go 431: Released address using handleID ContainerID="e26a9a900cfa9ccc906f8c6a7819014f50531adfeeae7d5ef67d9dc6d0683210" HandleID="k8s-pod-network.e26a9a900cfa9ccc906f8c6a7819014f50531adfeeae7d5ef67d9dc6d0683210" Workload="minikube-k8s-storage--app--deployment--58d8dfb886--8ssbz-eth0"
Jul 14 09:56:43 minikube cri-dockerd[1448]: 2025-07-14 09:56:43.008 [INFO][8174] ipam/ipam_plugin.go 440: Releasing address using workloadID ContainerID="e26a9a900cfa9ccc906f8c6a7819014f50531adfeeae7d5ef67d9dc6d0683210" HandleID="k8s-pod-network.e26a9a900cfa9ccc906f8c6a7819014f50531adfeeae7d5ef67d9dc6d0683210" Workload="minikube-k8s-storage--app--deployment--58d8dfb886--8ssbz-eth0"
Jul 14 09:56:43 minikube cri-dockerd[1448]: 2025-07-14 09:56:43.009 [INFO][8174] ipam/ipam_plugin.go 374: Released host-wide IPAM lock.
Jul 14 09:56:43 minikube cri-dockerd[1448]: 2025-07-14 09:56:43.009 [INFO][8159] cni-plugin/k8s.go 653: Teardown processing complete. ContainerID="e26a9a900cfa9ccc906f8c6a7819014f50531adfeeae7d5ef67d9dc6d0683210"
Jul 14 09:56:43 minikube dockerd[1582]: time="2025-07-14T09:56:43.038011326Z" level=info msg="ignoring event" container=e26a9a900cfa9ccc906f8c6a7819014f50531adfeeae7d5ef67d9dc6d0683210 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 14 09:56:43 minikube dockerd[1589]: time="2025-07-14T09:56:43.039081952Z" level=info msg="shim disconnected" id=e26a9a900cfa9ccc906f8c6a7819014f50531adfeeae7d5ef67d9dc6d0683210 namespace=moby
Jul 14 09:56:43 minikube dockerd[1589]: time="2025-07-14T09:56:43.039142660Z" level=warning msg="cleaning up after shim disconnected" id=e26a9a900cfa9ccc906f8c6a7819014f50531adfeeae7d5ef67d9dc6d0683210 namespace=moby
Jul 14 09:56:43 minikube dockerd[1589]: time="2025-07-14T09:56:43.039248702Z" level=info msg="cleaning up dead shim" namespace=moby
Jul 14 09:56:47 minikube dockerd[1589]: time="2025-07-14T09:56:47.166781038Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jul 14 09:56:47 minikube dockerd[1589]: time="2025-07-14T09:56:47.166850080Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jul 14 09:56:47 minikube dockerd[1589]: time="2025-07-14T09:56:47.166856872Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jul 14 09:56:47 minikube dockerd[1589]: time="2025-07-14T09:56:47.166906830Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jul 14 09:56:47 minikube cri-dockerd[1448]: time="2025-07-14T09:56:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.287 [INFO][8301] cni-plugin/plugin.go 340: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {minikube-k8s-storage--app--deployment--58d8dfb886--g89xl-eth0 storage-app-deployment-58d8dfb886- default  8611148f-7d5b-40e9-a7a9-9f820609a737 822 0 2025-07-14 09:56:46 +0000 UTC <nil> <nil> map[app:storage-app pod-template-hash:58d8dfb886 projectcalico.org/namespace:default projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:default] map[] [] [] []} {k8s  minikube  storage-app-deployment-58d8dfb886-g89xl eth0 default [] []   [kns.default ksa.default.default] cali25f6ba76cf9  [] [] <nil>}} ContainerID="dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" Namespace="default" Pod="storage-app-deployment-58d8dfb886-g89xl" WorkloadEndpoint="minikube-k8s-storage--app--deployment--58d8dfb886--g89xl-"
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.287 [INFO][8301] cni-plugin/k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" Namespace="default" Pod="storage-app-deployment-58d8dfb886-g89xl" WorkloadEndpoint="minikube-k8s-storage--app--deployment--58d8dfb886--g89xl-eth0"
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.411 [INFO][8309] ipam/ipam_plugin.go 225: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" HandleID="k8s-pod-network.dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" Workload="minikube-k8s-storage--app--deployment--58d8dfb886--g89xl-eth0"
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.412 [INFO][8309] ipam/ipam_plugin.go 265: Auto assigning IP ContainerID="dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" HandleID="k8s-pod-network.dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" Workload="minikube-k8s-storage--app--deployment--58d8dfb886--g89xl-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0x40002cf5d0), Attrs:map[string]string{"namespace":"default", "node":"minikube", "pod":"storage-app-deployment-58d8dfb886-g89xl", "timestamp":"2025-07-14 09:56:47.411951123 +0000 UTC"}, Hostname:"minikube", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.412 [INFO][8309] ipam/ipam_plugin.go 353: About to acquire host-wide IPAM lock.
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.412 [INFO][8309] ipam/ipam_plugin.go 368: Acquired host-wide IPAM lock.
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.412 [INFO][8309] ipam/ipam.go 110: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'minikube'
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.417 [INFO][8309] ipam/ipam.go 691: Looking up existing affinities for host handle="k8s-pod-network.dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" host="minikube"
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.420 [INFO][8309] ipam/ipam.go 394: Looking up existing affinities for host host="minikube"
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.425 [INFO][8309] ipam/ipam.go 511: Trying affinity for 10.244.120.64/26 host="minikube"
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.427 [INFO][8309] ipam/ipam.go 158: Attempting to load block cidr=10.244.120.64/26 host="minikube"
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.428 [INFO][8309] ipam/ipam.go 235: Affinity is confirmed and block has been loaded cidr=10.244.120.64/26 host="minikube"
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.428 [INFO][8309] ipam/ipam.go 1220: Attempting to assign 1 addresses from block block=10.244.120.64/26 handle="k8s-pod-network.dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" host="minikube"
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.431 [INFO][8309] ipam/ipam.go 1764: Creating new handle: k8s-pod-network.dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.435 [INFO][8309] ipam/ipam.go 1243: Writing block in order to claim IPs block=10.244.120.64/26 handle="k8s-pod-network.dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" host="minikube"
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.438 [INFO][8309] ipam/ipam.go 1256: Successfully claimed IPs: [10.244.120.68/26] block=10.244.120.64/26 handle="k8s-pod-network.dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" host="minikube"
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.438 [INFO][8309] ipam/ipam.go 878: Auto-assigned 1 out of 1 IPv4s: [10.244.120.68/26] handle="k8s-pod-network.dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" host="minikube"
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.438 [INFO][8309] ipam/ipam_plugin.go 374: Released host-wide IPAM lock.
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.438 [INFO][8309] ipam/ipam_plugin.go 283: Calico CNI IPAM assigned addresses IPv4=[10.244.120.68/26] IPv6=[] ContainerID="dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" HandleID="k8s-pod-network.dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" Workload="minikube-k8s-storage--app--deployment--58d8dfb886--g89xl-eth0"
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.439 [INFO][8301] cni-plugin/k8s.go 418: Populated endpoint ContainerID="dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" Namespace="default" Pod="storage-app-deployment-58d8dfb886-g89xl" WorkloadEndpoint="minikube-k8s-storage--app--deployment--58d8dfb886--g89xl-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"minikube-k8s-storage--app--deployment--58d8dfb886--g89xl-eth0", GenerateName:"storage-app-deployment-58d8dfb886-", Namespace:"default", SelfLink:"", UID:"8611148f-7d5b-40e9-a7a9-9f820609a737", ResourceVersion:"822", Generation:0, CreationTimestamp:time.Date(2025, time.July, 14, 9, 56, 46, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"storage-app", "pod-template-hash":"58d8dfb886", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"minikube", ContainerID:"", Pod:"storage-app-deployment-58d8dfb886-g89xl", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.244.120.68/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali25f6ba76cf9", MAC:"", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil), QoSControls:(*v3.QoSControls)(nil)}}
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.439 [INFO][8301] cni-plugin/k8s.go 419: Calico CNI using IPs: [10.244.120.68/32] ContainerID="dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" Namespace="default" Pod="storage-app-deployment-58d8dfb886-g89xl" WorkloadEndpoint="minikube-k8s-storage--app--deployment--58d8dfb886--g89xl-eth0"
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.439 [INFO][8301] cni-plugin/dataplane_linux.go 69: Setting the host side veth name to cali25f6ba76cf9 ContainerID="dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" Namespace="default" Pod="storage-app-deployment-58d8dfb886-g89xl" WorkloadEndpoint="minikube-k8s-storage--app--deployment--58d8dfb886--g89xl-eth0"
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.445 [INFO][8301] cni-plugin/dataplane_linux.go 508: Disabling IPv4 forwarding ContainerID="dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" Namespace="default" Pod="storage-app-deployment-58d8dfb886-g89xl" WorkloadEndpoint="minikube-k8s-storage--app--deployment--58d8dfb886--g89xl-eth0"
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.445 [INFO][8301] cni-plugin/k8s.go 446: Added Mac, interface name, and active container ID to endpoint ContainerID="dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" Namespace="default" Pod="storage-app-deployment-58d8dfb886-g89xl" WorkloadEndpoint="minikube-k8s-storage--app--deployment--58d8dfb886--g89xl-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"minikube-k8s-storage--app--deployment--58d8dfb886--g89xl-eth0", GenerateName:"storage-app-deployment-58d8dfb886-", Namespace:"default", SelfLink:"", UID:"8611148f-7d5b-40e9-a7a9-9f820609a737", ResourceVersion:"822", Generation:0, CreationTimestamp:time.Date(2025, time.July, 14, 9, 56, 46, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"storage-app", "pod-template-hash":"58d8dfb886", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"minikube", ContainerID:"dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772", Pod:"storage-app-deployment-58d8dfb886-g89xl", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.244.120.68/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali25f6ba76cf9", MAC:"16:3c:c8:dc:c6:02", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil), QoSControls:(*v3.QoSControls)(nil)}}
Jul 14 09:56:47 minikube cri-dockerd[1448]: 2025-07-14 09:56:47.450 [INFO][8301] cni-plugin/k8s.go 532: Wrote updated endpoint to datastore ContainerID="dbcf7dafbd2562fc12555d063c3906a93fac7ea8cdb31c619bc0cba10c0c0772" Namespace="default" Pod="storage-app-deployment-58d8dfb886-g89xl" WorkloadEndpoint="minikube-k8s-storage--app--deployment--58d8dfb886--g89xl-eth0"
Jul 14 09:56:49 minikube dockerd[1582]: time="2025-07-14T09:56:49.635991217Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jul 14 09:56:49 minikube dockerd[1582]: time="2025-07-14T09:56:49.636128634Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jul 14 09:57:06 minikube dockerd[1582]: time="2025-07-14T09:57:06.849066272Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jul 14 09:57:06 minikube dockerd[1582]: time="2025-07-14T09:57:06.849186355Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jul 14 09:57:38 minikube dockerd[1582]: time="2025-07-14T09:57:38.993435825Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jul 14 09:57:38 minikube dockerd[1582]: time="2025-07-14T09:57:38.993895366Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE                                                                                             CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
c6df31b019094       calico/kube-controllers@sha256:e68a8df468e0adadfad8ada0b6c222ff0a5b0b15861d7a243ff506e1280b3ae2   3 minutes ago       Running             calico-kube-controllers   0                   060adcdf8e16f       calico-kube-controllers-7bfdc5b57c-6l97l
d5e11a1c29f52       f72407be9e08c                                                                                     3 minutes ago       Running             coredns                   0                   431905e7a0b3c       coredns-674b8bbfcf-zbh58
b91aae0a1f8c2       f7148fde8e28b                                                                                     3 minutes ago       Running             calico-node               0                   1215d85e27b15       calico-node-fklq6
b8c60f248c9c0       calico/node@sha256:7503abcb5222ab0a8de98c113471c3383c5d22c943a18dea455a95b9f99df021               3 minutes ago       Exited              mount-bpffs               0                   1215d85e27b15       calico-node-fklq6
9038471eba883       ba04bb24b9575                                                                                     3 minutes ago       Running             storage-provisioner       0                   bd2708665a7e1       storage-provisioner
17d934b4b69d8       0a1b3d5412de2                                                                                     3 minutes ago       Exited              install-cni               0                   1215d85e27b15       calico-node-fklq6
1517dd39080a8       calico/cni@sha256:d9c858d86e4fc4e26e98e98747bb8658da41f995cc0def9f46882cfa51d35c95                3 minutes ago       Exited              upgrade-ipam              0                   1215d85e27b15       calico-node-fklq6
f496804183d08       3e58848989f55                                                                                     4 minutes ago       Running             kube-proxy                0                   afcdc9b6d6e28       kube-proxy-fgdlq
460e0c71387a1       014094c90caac                                                                                     4 minutes ago       Running             kube-scheduler            0                   63f46f1ea52ed       kube-scheduler-minikube
3b7f5066b40b4       674996a72aa59                                                                                     4 minutes ago       Running             kube-controller-manager   0                   2fce3f93f3051       kube-controller-manager-minikube
a54d4d00400fc       31747a36ce712                                                                                     4 minutes ago       Running             etcd                      0                   0db30a0e83dab       etcd-minikube
6dfd948d3b532       9a2b7cf4f8540                                                                                     4 minutes ago       Running             kube-apiserver            0                   e1916527feaa1       kube-apiserver-minikube


==> coredns [d5e11a1c29f5] <==
maxprocs: Leaving GOMAXPROCS=2: CPU quota undefined
.:53
[INFO] plugin/reload: Running configuration SHA512 = 3409057fd96ced495c54dfbc11c46c37cafb7915196e2732d6ff3dd9cada5deb0c590f4db58bfc38d19787f246b979122fa34cab1d3a970e57ae724a4727661f
CoreDNS-1.12.0
linux/arm64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:45662 - 12441 "HINFO IN 3244924781975716736.1890383755562202059. udp 57 false 512" NXDOMAIN qr,rd,ra 132 2.005970834s
[INFO] 127.0.0.1:33710 - 57575 "HINFO IN 3244924781975716736.1890383755562202059. udp 57 false 512" NXDOMAIN qr,aa,rd,ra 132 0.000436708s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_07_14T18_53_44_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 192.168.105.3/24
                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.120.64
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 14 Jul 2025 09:53:40 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 14 Jul 2025 09:57:40 +0000
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Mon, 14 Jul 2025 09:54:09 +0000   Mon, 14 Jul 2025 09:54:09 +0000   CalicoIsUp                   Calico is running on this node
  MemoryPressure       False   Mon, 14 Jul 2025 09:54:45 +0000   Mon, 14 Jul 2025 09:53:39 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Mon, 14 Jul 2025 09:54:45 +0000   Mon, 14 Jul 2025 09:53:39 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Mon, 14 Jul 2025 09:54:45 +0000   Mon, 14 Jul 2025 09:53:39 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Mon, 14 Jul 2025 09:54:45 +0000   Mon, 14 Jul 2025 09:54:01 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.105.3
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17734596Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             3904736Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17734596Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             3904736Ki
  pods:               110
System Info:
  Machine ID:                 606a8bcf5a414e5a8c6036482fb9735b
  System UUID:                606a8bcf5a414e5a8c6036482fb9735b
  Boot ID:                    da77f3fd-6405-49f5-8a69-045ac3d312c6
  Kernel Version:             5.10.207
  OS Image:                   Buildroot 2025.02
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://28.0.4
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  default                     storage-app-deployment-58d8dfb886-g89xl     0 (0%)        0 (0%)      0 (0%)           0 (0%)         62s
  kube-system                 calico-kube-controllers-7bfdc5b57c-6l97l    0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m1s
  kube-system                 calico-node-fklq6                           250m (12%)    0 (0%)      0 (0%)           0 (0%)         4m
  kube-system                 coredns-674b8bbfcf-zbh58                    100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     4m1s
  kube-system                 etcd-minikube                               100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         4m6s
  kube-system                 kube-apiserver-minikube                     250m (12%)    0 (0%)      0 (0%)           0 (0%)         4m8s
  kube-system                 kube-controller-manager-minikube            200m (10%)    0 (0%)      0 (0%)           0 (0%)         4m6s
  kube-system                 kube-proxy-fgdlq                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m
  kube-system                 kube-scheduler-minikube                     100m (5%)     0 (0%)      0 (0%)           0 (0%)         4m6s
  kube-system                 storage-provisioner                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m4s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                1 (50%)     0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
  hugepages-32Mi     0 (0%)      0 (0%)
  hugepages-64Ki     0 (0%)      0 (0%)
Events:
  Type    Reason                   Age    From             Message
  ----    ------                   ----   ----             -------
  Normal  Starting                 3m59s  kube-proxy       
  Normal  Starting                 4m6s   kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  4m6s   kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  4m6s   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    4m6s   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     4m6s   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           4m1s   node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  NodeReady                3m47s  kubelet          Node minikube status is now: NodeReady


==> dmesg <==
[Jul14 09:53] ACPI: SRAT not present
[  +0.000137] KASLR disabled due to lack of seed
[  +0.000251] EINJ: EINJ table not found.
[  +0.010213] (rpcbind)[144]: rpcbind.service: Referenced but unset environment variable evaluates to an empty string: RPCBIND_OPTIONS
[  +0.000231] platform regulatory.0: Direct firmware load for regulatory.db failed with error -2
[  +1.074708] kauditd_printk_skb: 186 callbacks suppressed
[  +0.922043] kauditd_printk_skb: 118 callbacks suppressed
[  +4.837914] kauditd_printk_skb: 52 callbacks suppressed
[  +6.065962] kauditd_printk_skb: 51 callbacks suppressed
[Jul14 09:54] kauditd_printk_skb: 2 callbacks suppressed
[  +5.493987] kauditd_printk_skb: 70 callbacks suppressed
[  +5.337537] kauditd_printk_skb: 86 callbacks suppressed
[  +3.355120] kauditd_printk_skb: 1 callbacks suppressed
[Jul14 09:56] kauditd_printk_skb: 2 callbacks suppressed


==> etcd [a54d4d00400f] <==
{"level":"warn","ts":"2025-07-14T09:53:38.429250Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"warn","ts":"2025-07-14T09:53:38.429376Z","caller":"etcdmain/config.go:389","msg":"--proxy-refresh-interval is deprecated in 3.5 and will be decommissioned in 3.6."}
{"level":"info","ts":"2025-07-14T09:53:38.429425Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.105.3:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.105.3:2380","--initial-cluster=minikube=https://192.168.105.3:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.105.3:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.105.3:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2025-07-14T09:53:38.429513Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-07-14T09:53:38.429536Z","caller":"embed/etcd.go:140","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.105.3:2380"]}
{"level":"info","ts":"2025-07-14T09:53:38.429564Z","caller":"embed/etcd.go:528","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-07-14T09:53:38.429836Z","caller":"embed/etcd.go:148","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.105.3:2379"]}
{"level":"info","ts":"2025-07-14T09:53:38.429916Z","caller":"embed/etcd.go:323","msg":"starting an etcd server","etcd-version":"3.5.21","git-sha":"a17edfd59","go-version":"go1.23.7","go-os":"linux","go-arch":"arm64","max-cpu-set":2,"max-cpu-available":2,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.105.3:2380"],"listen-peer-urls":["https://192.168.105.3:2380"],"advertise-client-urls":["https://192.168.105.3:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.105.3:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.105.3:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-07-14T09:53:38.431289Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"1.222083ms"}
{"level":"info","ts":"2025-07-14T09:53:38.433243Z","caller":"etcdserver/raft.go:506","msg":"starting local member","local-member-id":"3903ed6124adcb9c","cluster-id":"fb2c353f1f2d6c72"}
{"level":"info","ts":"2025-07-14T09:53:38.433369Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"3903ed6124adcb9c switched to configuration voters=()"}
{"level":"info","ts":"2025-07-14T09:53:38.433433Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"3903ed6124adcb9c became follower at term 0"}
{"level":"info","ts":"2025-07-14T09:53:38.433453Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft 3903ed6124adcb9c [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-07-14T09:53:38.433466Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"3903ed6124adcb9c became follower at term 1"}
{"level":"info","ts":"2025-07-14T09:53:38.433503Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"3903ed6124adcb9c switched to configuration voters=(4108388286575004572)"}
{"level":"warn","ts":"2025-07-14T09:53:38.434713Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-07-14T09:53:38.434991Z","caller":"mvcc/kvstore.go:425","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-07-14T09:53:38.435026Z","caller":"etcdserver/server.go:628","msg":"restore consistentIndex","index":0}
{"level":"info","ts":"2025-07-14T09:53:38.435246Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-07-14T09:53:38.435661Z","caller":"etcdserver/server.go:875","msg":"starting etcd server","local-member-id":"3903ed6124adcb9c","local-server-version":"3.5.21","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-07-14T09:53:38.435873Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-14T09:53:38.437449Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-07-14T09:53:38.437645Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-14T09:53:38.437700Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-14T09:53:38.437720Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-14T09:53:38.437920Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"3903ed6124adcb9c switched to configuration voters=(4108388286575004572)"}
{"level":"info","ts":"2025-07-14T09:53:38.438132Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"3903ed6124adcb9c","initial-advertise-peer-urls":["https://192.168.105.3:2380"],"listen-peer-urls":["https://192.168.105.3:2380"],"advertise-client-urls":["https://192.168.105.3:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.105.3:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-07-14T09:53:38.438167Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-07-14T09:53:38.438190Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.105.3:2380"}
{"level":"info","ts":"2025-07-14T09:53:38.438205Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.105.3:2380"}
{"level":"info","ts":"2025-07-14T09:53:38.438400Z","caller":"etcdserver/server.go:759","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"3903ed6124adcb9c","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-07-14T09:53:38.438529Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fb2c353f1f2d6c72","local-member-id":"3903ed6124adcb9c","added-peer-id":"3903ed6124adcb9c","added-peer-peer-urls":["https://192.168.105.3:2380"],"added-peer-is-learner":false}
{"level":"info","ts":"2025-07-14T09:53:39.036755Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"3903ed6124adcb9c is starting a new election at term 1"}
{"level":"info","ts":"2025-07-14T09:53:39.037055Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"3903ed6124adcb9c became pre-candidate at term 1"}
{"level":"info","ts":"2025-07-14T09:53:39.037146Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"3903ed6124adcb9c received MsgPreVoteResp from 3903ed6124adcb9c at term 1"}
{"level":"info","ts":"2025-07-14T09:53:39.037234Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"3903ed6124adcb9c became candidate at term 2"}
{"level":"info","ts":"2025-07-14T09:53:39.037251Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"3903ed6124adcb9c received MsgVoteResp from 3903ed6124adcb9c at term 2"}
{"level":"info","ts":"2025-07-14T09:53:39.037264Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"3903ed6124adcb9c became leader at term 2"}
{"level":"info","ts":"2025-07-14T09:53:39.037272Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: 3903ed6124adcb9c elected leader 3903ed6124adcb9c at term 2"}
{"level":"info","ts":"2025-07-14T09:53:39.044860Z","caller":"etcdserver/server.go:2697","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-14T09:53:39.052946Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"3903ed6124adcb9c","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.105.3:2379]}","request-path":"/0/members/3903ed6124adcb9c/attributes","cluster-id":"fb2c353f1f2d6c72","publish-timeout":"7s"}
{"level":"info","ts":"2025-07-14T09:53:39.053225Z","caller":"membership/cluster.go:587","msg":"set initial cluster version","cluster-id":"fb2c353f1f2d6c72","local-member-id":"3903ed6124adcb9c","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-14T09:53:39.053270Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-14T09:53:39.053301Z","caller":"etcdserver/server.go:2721","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-14T09:53:39.053319Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-07-14T09:53:39.053359Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-07-14T09:53:39.053386Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-07-14T09:53:39.053423Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-07-14T09:53:39.053834Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-14T09:53:39.054142Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-07-14T09:53:39.054282Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-14T09:53:39.059498Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.105.3:2379"}


==> kernel <==
 09:57:48 up 4 min,  0 users,  load average: 0.22, 0.20, 0.09
Linux minikube 5.10.207 #1 SMP PREEMPT Thu May 15 21:49:29 UTC 2025 aarch64 GNU/Linux
PRETTY_NAME="Buildroot 2025.02"


==> kube-apiserver [6dfd948d3b53] <==
I0714 09:53:40.051219       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0714 09:53:40.054928       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0714 09:53:40.054970       1 policy_source.go:240] refreshing policies
I0714 09:53:40.065255       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0714 09:53:40.065295       1 aggregator.go:171] initial CRD sync complete...
I0714 09:53:40.065329       1 autoregister_controller.go:144] Starting autoregister controller
I0714 09:53:40.065339       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0714 09:53:40.065346       1 cache.go:39] Caches are synced for autoregister controller
E0714 09:53:40.067536       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
E0714 09:53:40.100728       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I0714 09:53:40.151001       1 controller.go:667] quota admission added evaluator for: namespaces
I0714 09:53:40.153829       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0714 09:53:40.153960       1 default_servicecidr_controller.go:214] Setting default ServiceCIDR condition Ready to True
I0714 09:53:40.156367       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0714 09:53:40.156391       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0714 09:53:40.269455       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0714 09:53:40.963604       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0714 09:53:40.969415       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0714 09:53:40.969431       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0714 09:53:41.094119       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0714 09:53:41.103155       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0714 09:53:41.153343       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0714 09:53:41.155476       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.105.3]
I0714 09:53:41.155818       1 controller.go:667] quota admission added evaluator for: endpoints
I0714 09:53:41.156969       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0714 09:53:42.023483       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0714 09:53:42.131190       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0714 09:53:42.134916       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0714 09:53:42.141116       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I0714 09:53:43.121606       1 controller.go:667] quota admission added evaluator for: poddisruptionbudgets.policy
I0714 09:53:43.219787       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.227964       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.231816       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.238994       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.246924       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.248898       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.320654       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.324546       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.325329       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.329796       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.334454       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.337747       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.340580       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.347063       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.369807       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.384757       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.412721       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.425246       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.476612       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.494677       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.518539       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.530006       1 handler.go:288] Adding GroupVersion crd.projectcalico.org v1 to ResourceManager
I0714 09:53:43.549506       1 handler.go:288] Adding GroupVersion policy.networking.k8s.io v1alpha1 to ResourceManager
I0714 09:53:43.602141       1 handler.go:288] Adding GroupVersion policy.networking.k8s.io v1alpha1 to ResourceManager
I0714 09:53:46.868163       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0714 09:53:47.208931       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0714 09:53:47.212903       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0714 09:53:48.017427       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I0714 09:54:22.050469       1 alloc.go:328] "allocated clusterIPs" service="default/storage-app-service" clusterIPs={"IPv4":"10.104.83.119"}
I0714 09:54:22.069811       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-controller-manager [3b7f5066b40b] <==
I0714 09:53:46.616206       1 controllermanager.go:778] "Started controller" controller="replicationcontroller-controller"
I0714 09:53:46.616757       1 replica_set.go:219] "Starting controller" logger="replicationcontroller-controller" name="replicationcontroller"
I0714 09:53:46.616793       1 shared_informer.go:350] "Waiting for caches to sync" controller="ReplicationController"
I0714 09:53:46.619464       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0714 09:53:46.657258       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0714 09:53:46.665578       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0714 09:53:46.665717       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0714 09:53:46.670250       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0714 09:53:46.671310       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0714 09:53:46.671635       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0714 09:53:46.672139       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0714 09:53:46.672255       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0714 09:53:46.672292       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0714 09:53:46.674341       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0714 09:53:46.674874       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0714 09:53:46.689473       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0714 09:53:46.703550       1 shared_informer.go:357] "Caches are synced" controller="job"
I0714 09:53:46.715883       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0714 09:53:46.715889       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0714 09:53:46.715959       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0714 09:53:46.717152       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0714 09:53:46.719012       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0714 09:53:46.720086       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0714 09:53:46.817045       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0714 09:53:46.820083       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0714 09:53:46.822176       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0714 09:53:46.869613       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0714 09:53:46.874858       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0714 09:53:46.912264       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0714 09:53:46.916428       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0714 09:53:46.916428       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0714 09:53:46.918040       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0714 09:53:46.918057       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0714 09:53:46.918187       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0714 09:53:46.973010       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0714 09:53:46.984549       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0714 09:53:47.016268       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0714 09:53:47.017345       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0714 09:53:47.017344       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0714 09:53:47.019707       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0714 09:53:47.035462       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0714 09:53:47.047089       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0714 09:53:47.066316       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0714 09:53:47.066380       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0714 09:53:47.066454       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0714 09:53:47.066501       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0714 09:53:47.068896       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0714 09:53:47.070847       1 shared_informer.go:357] "Caches are synced" controller="node"
I0714 09:53:47.070919       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0714 09:53:47.070939       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0714 09:53:47.070942       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0714 09:53:47.070944       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0714 09:53:47.076998       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0714 09:53:47.455502       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0714 09:53:47.455546       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0714 09:53:47.455559       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0714 09:53:47.463257       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0714 09:54:04.629331       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I0714 09:54:20.032853       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0714 09:54:20.133556       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"


==> kube-proxy [f496804183d0] <==
I0714 09:53:48.705535       1 server_linux.go:63] "Using iptables proxy"
E0714 09:53:48.715215       1 proxier.go:732] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-24: Error: Could not process rule: Operation not supported
	add table ip kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^
 >
E0714 09:53:48.718458       1 proxier.go:732] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-25: Error: Could not process rule: Operation not supported
	add table ip6 kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^^
 >
I0714 09:53:48.724637       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.105.3"]
E0714 09:53:48.724676       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0714 09:53:48.732525       1 server_linux.go:122] "No iptables support for family" ipFamily="IPv6"
I0714 09:53:48.732539       1 server.go:256] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I0714 09:53:48.732558       1 server_linux.go:145] "Using iptables Proxier"
I0714 09:53:48.734279       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0714 09:53:48.734425       1 server.go:516] "Version info" version="v1.33.1"
I0714 09:53:48.734432       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0714 09:53:48.736102       1 config.go:199] "Starting service config controller"
I0714 09:53:48.736145       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0714 09:53:48.736169       1 config.go:105] "Starting endpoint slice config controller"
I0714 09:53:48.736181       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0714 09:53:48.736208       1 config.go:440] "Starting serviceCIDR config controller"
I0714 09:53:48.736222       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0714 09:53:48.736249       1 config.go:329] "Starting node config controller"
I0714 09:53:48.736260       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0714 09:53:48.836763       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0714 09:53:48.836784       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0714 09:53:48.836790       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0714 09:53:48.836768       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"


==> kube-scheduler [460e0c71387a] <==
I0714 09:53:39.402159       1 serving.go:386] Generated self-signed cert in-memory
W0714 09:53:40.000986       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0714 09:53:40.001024       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0714 09:53:40.001032       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0714 09:53:40.001039       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0714 09:53:40.021279       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0714 09:53:40.021317       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0714 09:53:40.026871       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0714 09:53:40.027350       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0714 09:53:40.027553       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0714 09:53:40.027945       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0714 09:53:40.030715       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0714 09:53:40.030715       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0714 09:53:40.030745       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0714 09:53:40.030768       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_arm64.s:1223" type="*v1.ConfigMap"
E0714 09:53:40.030936       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0714 09:53:40.030952       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0714 09:53:40.030973       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0714 09:53:40.030987       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0714 09:53:40.031412       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0714 09:53:40.031437       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0714 09:53:40.031487       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0714 09:53:40.031504       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0714 09:53:40.031515       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0714 09:53:40.031776       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0714 09:53:40.031963       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0714 09:53:40.032034       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0714 09:53:40.938202       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0714 09:53:40.955365       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0714 09:53:40.978161       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0714 09:53:40.997875       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0714 09:53:41.034502       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
I0714 09:53:41.628521       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Jul 14 09:54:09 minikube kubelet[2533]: E0714 09:54:09.372318    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"calico-kube-controllers-7bfdc5b57c-6l97l_kube-system(f7a4d334-b28a-4ee4-8cce-982cc90a8a63)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"calico-kube-controllers-7bfdc5b57c-6l97l_kube-system(f7a4d334-b28a-4ee4-8cce-982cc90a8a63)\\\": rpc error: code = Unknown desc = failed to set up sandbox container \\\"ba655c7c47b46fa7519699c0f1c0d672f7a6eb7646cd92b1c443c83798e82cd6\\\" network for pod \\\"calico-kube-controllers-7bfdc5b57c-6l97l\\\": networkPlugin cni failed to set up pod \\\"calico-kube-controllers-7bfdc5b57c-6l97l_kube-system\\\" network: plugin type=\\\"calico\\\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\"" pod="kube-system/calico-kube-controllers-7bfdc5b57c-6l97l" podUID="f7a4d334-b28a-4ee4-8cce-982cc90a8a63"
Jul 14 09:54:09 minikube kubelet[2533]: E0714 09:54:09.406987    2533 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"d2975eb1ff3e15e1087f1c1ba0d78d231ec400a77c94ecd7405818b3a4814d56\" network for pod \"coredns-674b8bbfcf-zbh58\": networkPlugin cni failed to set up pod \"coredns-674b8bbfcf-zbh58_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/"
Jul 14 09:54:09 minikube kubelet[2533]: E0714 09:54:09.407025    2533 kuberuntime_sandbox.go:70] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to set up sandbox container \"d2975eb1ff3e15e1087f1c1ba0d78d231ec400a77c94ecd7405818b3a4814d56\" network for pod \"coredns-674b8bbfcf-zbh58\": networkPlugin cni failed to set up pod \"coredns-674b8bbfcf-zbh58_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/coredns-674b8bbfcf-zbh58"
Jul 14 09:54:09 minikube kubelet[2533]: E0714 09:54:09.407039    2533 kuberuntime_manager.go:1252] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to set up sandbox container \"d2975eb1ff3e15e1087f1c1ba0d78d231ec400a77c94ecd7405818b3a4814d56\" network for pod \"coredns-674b8bbfcf-zbh58\": networkPlugin cni failed to set up pod \"coredns-674b8bbfcf-zbh58_kube-system\" network: plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/" pod="kube-system/coredns-674b8bbfcf-zbh58"
Jul 14 09:54:09 minikube kubelet[2533]: E0714 09:54:09.407100    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"coredns-674b8bbfcf-zbh58_kube-system(3e88ef5a-8b58-41ad-bbc4-95869d5a8a2f)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"coredns-674b8bbfcf-zbh58_kube-system(3e88ef5a-8b58-41ad-bbc4-95869d5a8a2f)\\\": rpc error: code = Unknown desc = failed to set up sandbox container \\\"d2975eb1ff3e15e1087f1c1ba0d78d231ec400a77c94ecd7405818b3a4814d56\\\" network for pod \\\"coredns-674b8bbfcf-zbh58\\\": networkPlugin cni failed to set up pod \\\"coredns-674b8bbfcf-zbh58_kube-system\\\" network: plugin type=\\\"calico\\\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\"" pod="kube-system/coredns-674b8bbfcf-zbh58" podUID="3e88ef5a-8b58-41ad-bbc4-95869d5a8a2f"
Jul 14 09:54:10 minikube kubelet[2533]: I0714 09:54:10.233814    2533 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="ba655c7c47b46fa7519699c0f1c0d672f7a6eb7646cd92b1c443c83798e82cd6"
Jul 14 09:54:10 minikube kubelet[2533]: I0714 09:54:10.246767    2533 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d2975eb1ff3e15e1087f1c1ba0d78d231ec400a77c94ecd7405818b3a4814d56"
Jul 14 09:54:10 minikube kubelet[2533]: I0714 09:54:10.266967    2533 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/calico-node-fklq6" podStartSLOduration=4.723973784 podStartE2EDuration="22.266901667s" podCreationTimestamp="2025-07-14 09:53:48 +0000 UTC" firstStartedPulling="2025-07-14 09:53:48.630247264 +0000 UTC m=+6.701002504" lastFinishedPulling="2025-07-14 09:54:08.730909833 +0000 UTC m=+24.243930387" observedRunningTime="2025-07-14 09:54:10.266729958 +0000 UTC m=+25.779750471" watchObservedRunningTime="2025-07-14 09:54:10.266901667 +0000 UTC m=+25.779922221"
Jul 14 09:54:12 minikube kubelet[2533]: I0714 09:54:12.297176    2533 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-674b8bbfcf-zbh58" podStartSLOduration=25.297109126 podStartE2EDuration="25.297109126s" podCreationTimestamp="2025-07-14 09:53:47 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-07-14 09:54:11.317907125 +0000 UTC m=+26.830927680" watchObservedRunningTime="2025-07-14 09:54:12.297109126 +0000 UTC m=+27.810129681"
Jul 14 09:54:16 minikube kubelet[2533]: I0714 09:54:16.370720    2533 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/calico-kube-controllers-7bfdc5b57c-6l97l" podStartSLOduration=24.254595584 podStartE2EDuration="29.370632503s" podCreationTimestamp="2025-07-14 09:53:47 +0000 UTC" firstStartedPulling="2025-07-14 09:54:10.684213458 +0000 UTC m=+26.197234055" lastFinishedPulling="2025-07-14 09:54:15.800250378 +0000 UTC m=+31.313270974" observedRunningTime="2025-07-14 09:54:16.339375545 +0000 UTC m=+31.852396099" watchObservedRunningTime="2025-07-14 09:54:16.370632503 +0000 UTC m=+31.883653058"
Jul 14 09:54:18 minikube kubelet[2533]: I0714 09:54:18.345968    2533 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-01ac01eb-75ff-403a-b9de-de10ef1d2032\" (UniqueName: \"kubernetes.io/host-path/172b0f07-ce46-4358-9a65-901ceb57f7d7-pvc-01ac01eb-75ff-403a-b9de-de10ef1d2032\") pod \"storage-app-deployment-58d8dfb886-8ssbz\" (UID: \"172b0f07-ce46-4358-9a65-901ceb57f7d7\") " pod="default/storage-app-deployment-58d8dfb886-8ssbz"
Jul 14 09:54:18 minikube kubelet[2533]: I0714 09:54:18.346008    2533 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-cs4xd\" (UniqueName: \"kubernetes.io/projected/172b0f07-ce46-4358-9a65-901ceb57f7d7-kube-api-access-cs4xd\") pod \"storage-app-deployment-58d8dfb886-8ssbz\" (UID: \"172b0f07-ce46-4358-9a65-901ceb57f7d7\") " pod="default/storage-app-deployment-58d8dfb886-8ssbz"
Jul 14 09:54:20 minikube kubelet[2533]: E0714 09:54:20.909225    2533 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="storage-app:latest"
Jul 14 09:54:20 minikube kubelet[2533]: E0714 09:54:20.909345    2533 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="storage-app:latest"
Jul 14 09:54:20 minikube kubelet[2533]: E0714 09:54:20.910289    2533 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:storage-app,Image:storage-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:storage,ReadOnly:false,MountPath:/data,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-cs4xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-app-deployment-58d8dfb886-8ssbz_default(172b0f07-ce46-4358-9a65-901ceb57f7d7): ErrImagePull: Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jul 14 09:54:20 minikube kubelet[2533]: E0714 09:54:20.911505    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-app\" with ErrImagePull: \"Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/storage-app-deployment-58d8dfb886-8ssbz" podUID="172b0f07-ce46-4358-9a65-901ceb57f7d7"
Jul 14 09:54:21 minikube kubelet[2533]: E0714 09:54:21.359343    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-app\" with ImagePullBackOff: \"Back-off pulling image \\\"storage-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/storage-app-deployment-58d8dfb886-8ssbz" podUID="172b0f07-ce46-4358-9a65-901ceb57f7d7"
Jul 14 09:54:33 minikube kubelet[2533]: E0714 09:54:33.796135    2533 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="storage-app:latest"
Jul 14 09:54:33 minikube kubelet[2533]: E0714 09:54:33.796446    2533 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="storage-app:latest"
Jul 14 09:54:33 minikube kubelet[2533]: E0714 09:54:33.796602    2533 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:storage-app,Image:storage-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:storage,ReadOnly:false,MountPath:/data,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-cs4xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-app-deployment-58d8dfb886-8ssbz_default(172b0f07-ce46-4358-9a65-901ceb57f7d7): ErrImagePull: Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jul 14 09:54:33 minikube kubelet[2533]: E0714 09:54:33.797991    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-app\" with ErrImagePull: \"Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/storage-app-deployment-58d8dfb886-8ssbz" podUID="172b0f07-ce46-4358-9a65-901ceb57f7d7"
Jul 14 09:54:46 minikube kubelet[2533]: E0714 09:54:46.631043    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-app\" with ImagePullBackOff: \"Back-off pulling image \\\"storage-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/storage-app-deployment-58d8dfb886-8ssbz" podUID="172b0f07-ce46-4358-9a65-901ceb57f7d7"
Jul 14 09:55:00 minikube kubelet[2533]: E0714 09:55:00.889948    2533 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="storage-app:latest"
Jul 14 09:55:00 minikube kubelet[2533]: E0714 09:55:00.890105    2533 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="storage-app:latest"
Jul 14 09:55:00 minikube kubelet[2533]: E0714 09:55:00.891097    2533 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:storage-app,Image:storage-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:storage,ReadOnly:false,MountPath:/data,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-cs4xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-app-deployment-58d8dfb886-8ssbz_default(172b0f07-ce46-4358-9a65-901ceb57f7d7): ErrImagePull: Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jul 14 09:55:00 minikube kubelet[2533]: E0714 09:55:00.892280    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-app\" with ErrImagePull: \"Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/storage-app-deployment-58d8dfb886-8ssbz" podUID="172b0f07-ce46-4358-9a65-901ceb57f7d7"
Jul 14 09:55:11 minikube kubelet[2533]: E0714 09:55:11.633242    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-app\" with ImagePullBackOff: \"Back-off pulling image \\\"storage-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/storage-app-deployment-58d8dfb886-8ssbz" podUID="172b0f07-ce46-4358-9a65-901ceb57f7d7"
Jul 14 09:55:24 minikube kubelet[2533]: E0714 09:55:24.631214    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-app\" with ImagePullBackOff: \"Back-off pulling image \\\"storage-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/storage-app-deployment-58d8dfb886-8ssbz" podUID="172b0f07-ce46-4358-9a65-901ceb57f7d7"
Jul 14 09:55:37 minikube kubelet[2533]: E0714 09:55:37.628476    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-app\" with ImagePullBackOff: \"Back-off pulling image \\\"storage-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/storage-app-deployment-58d8dfb886-8ssbz" podUID="172b0f07-ce46-4358-9a65-901ceb57f7d7"
Jul 14 09:55:51 minikube kubelet[2533]: E0714 09:55:51.882651    2533 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="storage-app:latest"
Jul 14 09:55:51 minikube kubelet[2533]: E0714 09:55:51.882853    2533 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="storage-app:latest"
Jul 14 09:55:51 minikube kubelet[2533]: E0714 09:55:51.883185    2533 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:storage-app,Image:storage-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:storage,ReadOnly:false,MountPath:/data,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-cs4xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-app-deployment-58d8dfb886-8ssbz_default(172b0f07-ce46-4358-9a65-901ceb57f7d7): ErrImagePull: Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jul 14 09:55:51 minikube kubelet[2533]: E0714 09:55:51.887405    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-app\" with ErrImagePull: \"Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/storage-app-deployment-58d8dfb886-8ssbz" podUID="172b0f07-ce46-4358-9a65-901ceb57f7d7"
Jul 14 09:56:06 minikube kubelet[2533]: E0714 09:56:06.629811    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-app\" with ImagePullBackOff: \"Back-off pulling image \\\"storage-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/storage-app-deployment-58d8dfb886-8ssbz" podUID="172b0f07-ce46-4358-9a65-901ceb57f7d7"
Jul 14 09:56:19 minikube kubelet[2533]: E0714 09:56:19.628975    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-app\" with ImagePullBackOff: \"Back-off pulling image \\\"storage-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/storage-app-deployment-58d8dfb886-8ssbz" podUID="172b0f07-ce46-4358-9a65-901ceb57f7d7"
Jul 14 09:56:31 minikube kubelet[2533]: E0714 09:56:31.636774    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-app\" with ImagePullBackOff: \"Back-off pulling image \\\"storage-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/storage-app-deployment-58d8dfb886-8ssbz" podUID="172b0f07-ce46-4358-9a65-901ceb57f7d7"
Jul 14 09:56:42 minikube kubelet[2533]: E0714 09:56:42.628975    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-app\" with ImagePullBackOff: \"Back-off pulling image \\\"storage-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/storage-app-deployment-58d8dfb886-8ssbz" podUID="172b0f07-ce46-4358-9a65-901ceb57f7d7"
Jul 14 09:56:43 minikube kubelet[2533]: I0714 09:56:43.235285    2533 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-cs4xd\" (UniqueName: \"kubernetes.io/projected/172b0f07-ce46-4358-9a65-901ceb57f7d7-kube-api-access-cs4xd\") pod \"172b0f07-ce46-4358-9a65-901ceb57f7d7\" (UID: \"172b0f07-ce46-4358-9a65-901ceb57f7d7\") "
Jul 14 09:56:43 minikube kubelet[2533]: I0714 09:56:43.235306    2533 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"storage\" (UniqueName: \"kubernetes.io/host-path/172b0f07-ce46-4358-9a65-901ceb57f7d7-pvc-01ac01eb-75ff-403a-b9de-de10ef1d2032\") pod \"172b0f07-ce46-4358-9a65-901ceb57f7d7\" (UID: \"172b0f07-ce46-4358-9a65-901ceb57f7d7\") "
Jul 14 09:56:43 minikube kubelet[2533]: I0714 09:56:43.235599    2533 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/172b0f07-ce46-4358-9a65-901ceb57f7d7-pvc-01ac01eb-75ff-403a-b9de-de10ef1d2032" (OuterVolumeSpecName: "storage") pod "172b0f07-ce46-4358-9a65-901ceb57f7d7" (UID: "172b0f07-ce46-4358-9a65-901ceb57f7d7"). InnerVolumeSpecName "pvc-01ac01eb-75ff-403a-b9de-de10ef1d2032". PluginName "kubernetes.io/host-path", VolumeGIDValue ""
Jul 14 09:56:43 minikube kubelet[2533]: I0714 09:56:43.238486    2533 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/172b0f07-ce46-4358-9a65-901ceb57f7d7-kube-api-access-cs4xd" (OuterVolumeSpecName: "kube-api-access-cs4xd") pod "172b0f07-ce46-4358-9a65-901ceb57f7d7" (UID: "172b0f07-ce46-4358-9a65-901ceb57f7d7"). InnerVolumeSpecName "kube-api-access-cs4xd". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Jul 14 09:56:43 minikube kubelet[2533]: I0714 09:56:43.335549    2533 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-cs4xd\" (UniqueName: \"kubernetes.io/projected/172b0f07-ce46-4358-9a65-901ceb57f7d7-kube-api-access-cs4xd\") on node \"minikube\" DevicePath \"\""
Jul 14 09:56:43 minikube kubelet[2533]: I0714 09:56:43.335575    2533 reconciler_common.go:299] "Volume detached for volume \"pvc-01ac01eb-75ff-403a-b9de-de10ef1d2032\" (UniqueName: \"kubernetes.io/host-path/172b0f07-ce46-4358-9a65-901ceb57f7d7-pvc-01ac01eb-75ff-403a-b9de-de10ef1d2032\") on node \"minikube\" DevicePath \"\""
Jul 14 09:56:44 minikube kubelet[2533]: I0714 09:56:44.637889    2533 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="172b0f07-ce46-4358-9a65-901ceb57f7d7" path="/var/lib/kubelet/pods/172b0f07-ce46-4358-9a65-901ceb57f7d7/volumes"
Jul 14 09:56:46 minikube kubelet[2533]: I0714 09:56:46.959095    2533 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-01ac01eb-75ff-403a-b9de-de10ef1d2032\" (UniqueName: \"kubernetes.io/host-path/8611148f-7d5b-40e9-a7a9-9f820609a737-pvc-01ac01eb-75ff-403a-b9de-de10ef1d2032\") pod \"storage-app-deployment-58d8dfb886-g89xl\" (UID: \"8611148f-7d5b-40e9-a7a9-9f820609a737\") " pod="default/storage-app-deployment-58d8dfb886-g89xl"
Jul 14 09:56:46 minikube kubelet[2533]: I0714 09:56:46.959143    2533 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-z54j2\" (UniqueName: \"kubernetes.io/projected/8611148f-7d5b-40e9-a7a9-9f820609a737-kube-api-access-z54j2\") pod \"storage-app-deployment-58d8dfb886-g89xl\" (UID: \"8611148f-7d5b-40e9-a7a9-9f820609a737\") " pod="default/storage-app-deployment-58d8dfb886-g89xl"
Jul 14 09:56:49 minikube kubelet[2533]: E0714 09:56:49.642751    2533 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="storage-app:latest"
Jul 14 09:56:49 minikube kubelet[2533]: E0714 09:56:49.642911    2533 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="storage-app:latest"
Jul 14 09:56:49 minikube kubelet[2533]: E0714 09:56:49.643326    2533 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:storage-app,Image:storage-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:storage,ReadOnly:false,MountPath:/data,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-z54j2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-app-deployment-58d8dfb886-g89xl_default(8611148f-7d5b-40e9-a7a9-9f820609a737): ErrImagePull: Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jul 14 09:56:49 minikube kubelet[2533]: E0714 09:56:49.644738    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-app\" with ErrImagePull: \"Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/storage-app-deployment-58d8dfb886-g89xl" podUID="8611148f-7d5b-40e9-a7a9-9f820609a737"
Jul 14 09:56:49 minikube kubelet[2533]: E0714 09:56:49.692011    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-app\" with ImagePullBackOff: \"Back-off pulling image \\\"storage-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/storage-app-deployment-58d8dfb886-g89xl" podUID="8611148f-7d5b-40e9-a7a9-9f820609a737"
Jul 14 09:57:06 minikube kubelet[2533]: E0714 09:57:06.851616    2533 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="storage-app:latest"
Jul 14 09:57:06 minikube kubelet[2533]: E0714 09:57:06.851716    2533 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="storage-app:latest"
Jul 14 09:57:06 minikube kubelet[2533]: E0714 09:57:06.852255    2533 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:storage-app,Image:storage-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:storage,ReadOnly:false,MountPath:/data,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-z54j2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-app-deployment-58d8dfb886-g89xl_default(8611148f-7d5b-40e9-a7a9-9f820609a737): ErrImagePull: Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jul 14 09:57:06 minikube kubelet[2533]: E0714 09:57:06.853377    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-app\" with ErrImagePull: \"Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/storage-app-deployment-58d8dfb886-g89xl" podUID="8611148f-7d5b-40e9-a7a9-9f820609a737"
Jul 14 09:57:21 minikube kubelet[2533]: E0714 09:57:21.639790    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-app\" with ImagePullBackOff: \"Back-off pulling image \\\"storage-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/storage-app-deployment-58d8dfb886-g89xl" podUID="8611148f-7d5b-40e9-a7a9-9f820609a737"
Jul 14 09:57:39 minikube kubelet[2533]: E0714 09:57:39.007300    2533 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="storage-app:latest"
Jul 14 09:57:39 minikube kubelet[2533]: E0714 09:57:39.007609    2533 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="storage-app:latest"
Jul 14 09:57:39 minikube kubelet[2533]: E0714 09:57:39.008476    2533 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:storage-app,Image:storage-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:storage,ReadOnly:false,MountPath:/data,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-z54j2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-app-deployment-58d8dfb886-g89xl_default(8611148f-7d5b-40e9-a7a9-9f820609a737): ErrImagePull: Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jul 14 09:57:39 minikube kubelet[2533]: E0714 09:57:39.010573    2533 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-app\" with ErrImagePull: \"Error response from daemon: pull access denied for storage-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/storage-app-deployment-58d8dfb886-g89xl" podUID="8611148f-7d5b-40e9-a7a9-9f820609a737"


==> storage-provisioner [9038471eba88] <==
W0714 09:56:48.273233       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:56:48.276985       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:56:50.291596       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:56:50.295475       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:56:52.301313       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:56:52.308874       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:56:54.316625       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:56:54.321828       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:56:56.325578       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:56:56.330996       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:56:58.340365       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:56:58.345421       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:00.348174       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:00.351273       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:02.356674       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:02.364749       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:04.375881       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:04.382611       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:06.385539       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:06.388211       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:08.392682       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:08.397162       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:10.402986       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:10.405207       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:12.412435       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:12.418476       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:14.420913       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:14.425282       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:16.444010       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:16.476495       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:18.480550       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:18.485345       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:20.491425       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:20.496498       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:22.505748       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:22.522659       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:24.543999       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:24.554847       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:26.571963       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:26.585174       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:28.590681       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:28.595419       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:31.040037       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:31.091091       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:33.095193       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:33.098940       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:35.105854       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:35.116033       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:37.120010       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:37.122895       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:39.127070       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:39.130710       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:41.136378       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:41.139331       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:43.142687       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:43.145182       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:45.147471       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:45.150839       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:47.160794       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0714 09:57:47.165198       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

